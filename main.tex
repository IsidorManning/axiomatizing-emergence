%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  Axiomatizing AI: Emergence Under Scaling
%  Author: Isidor Manning
%  Clean & Structured LaTeX Header (2025)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[12pt]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% --- PAGE GEOMETRY & LAYOUT ---
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[
  margin=1.5in,
  includefoot
]{geometry}

\usepackage{setspace}        % Line spacing control
\usepackage{parskip}         % No paragraph indentation, vertical space between paragraphs
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% --- TYPOGRAPHY & READABILITY ---
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{microtype} % better kerning, spacing, line breaks
\linespread{1.05}      % slight line spacing to breathe

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% --- BIBLIOGRAPHY SETTINGS ---
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{natbib}
\bibliographystyle{plainnat}
\usepackage{doi}
\usepackage{url}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% --- ENUMERATIONS, TABLES, AND GRAPHICS ---
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{enumitem}        % Compact lists
\usepackage{booktabs}        % Professional tables
\usepackage{graphicx}        % Figures
\graphicspath{{figures/}}    % Default figure path
\usepackage[font=small,labelfont=bf,labelsep=period]{caption}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% --- MATHEMATICS PACKAGES ---
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage{varwidth}        % Helps with flexible box widths in theorem boxes
\usepackage{newtxtext,newtxmath}

\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Prob}{\text{Prob}}
\newcommand{\B}{\mathscr{B}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% --- HYPERLINKS & CROSS-REFERENCING ---
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[colorlinks=true,
            linkcolor=blue!40!black,
            citecolor=blue!50!black,
            urlcolor=blue!60!black,
            pdfusetitle,
            pagebackref=true]{hyperref}

\usepackage[capitalize,nameinlink]{cleveref}
% Examples:
%   \cref{sec:intro} -> “Section 1”
%   \cref{thm:main} -> “Theorem 3.2”
%   \cref{app:proofs} -> “Appendix A”

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% --- TCOLORBOX STYLES (Definitions, Theorems, etc.) ---
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[most]{tcolorbox}
\tcbuselibrary{theorems}
\tcbuselibrary{skins}

% General theorem box style
\tcbset{
  crispbox/.style={
    enhanced, breakable,
    colback=gray!14,
    colframe=black!35,
    boxrule=0.4pt,
    arc=0pt,
    left=8pt,right=8pt,top=6pt,bottom=6pt,
    before skip=10pt, after skip=10pt,
    attach boxed title to top left={yshift=-1mm},
    boxed title style={
      colback=gray!14,
      colframe=black!35,
      boxrule=0.4pt,
      arc=0pt,
      left=6pt,right=6pt,top=2pt,bottom=2pt,
    },
    varwidth boxed title,
    fonttitle=\bfseries\small,
    coltitle=black,
    separator sign={.\ },
  }
}

% --- amsthm theorems (labels + [Title] work out of the box)
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{example}{Example}[section]
\newtheorem{note}{Note}[section]
\newtheorem{axiom}{Axiom} % independent axiom counter

% --- Make cleveref print nice names
\crefname{definition}{Definition}{Definitions}
\crefname{theorem}{Theorem}{Theorems}
\crefname{lemma}{Lemma}{Lemmas}
\crefname{corollary}{Corollary}{Corollaries}
\crefname{example}{Example}{Examples}
\crefname{note}{Note}{Notes}
\crefname{axiom}{Axiom}{Axioms}

% --- Box the environments with your tcolorbox style
\tcolorboxenvironment{definition}{crispbox}
\tcolorboxenvironment{theorem}{crispbox}
\tcolorboxenvironment{lemma}{crispbox}
\tcolorboxenvironment{corollary}{crispbox}
\tcolorboxenvironment{example}{crispbox}
\tcolorboxenvironment{note}{crispbox}
\tcolorboxenvironment{axiom}{crispbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% --- CUSTOM MACROS ---
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Historical Axiom Entry: for version tracking of evolving statements
\newcommand{\HistAxiomEntry}[6]{%
  \begin{histaxiom}[#1]
    \textbf{Status:} #2\par
    \textbf{Date:} #3\par\medskip
    \textbf{Old statement.}\par #4\par\medskip
    \textbf{Issue.}\par #5\par\medskip
    \textbf{Solution.}\par #6
  \end{histaxiom}%
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% --- DOCUMENT METADATA ---
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Axiomatic Theory of Generalization and Structure Formation in Learning Systems}
\author{Isidor Seppälä Manning}
\date{August 2025}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% --- DOCUMENT START ---
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\maketitle

% === Abstract ===
\begin{abstract}
The recent successes of artificial intelligence—systems that acquire reasoning, language, and perception capabilities through scale—have revealed a profound gap in our theoretical understanding. While we can train such systems, we cannot yet explain, in mathematical terms, why new abilities emerge when scale and structure increase.

This work proposes an axiomatic theory of learning systems: a formal framework seeking the minimal assumptions under which learning, generalization, and emergent capability can arise in computational systems. The theory begins by defining a domain of learning systems equipped with topological and probabilistic structure, describing how data distributions, architectural procedures, and resource configurations jointly determine a system’s behavior. Within this domain, we formalize capabilities as measurable functionals over predictor distributions and define their expectation over training randomness as the capability measure $m_\Phi(S,R)$ 

From a small set of falsifiable axioms—continuity, representability, universality, and criticality—follow testable predictions about scaling laws, threshold phenomena, and invariance structures observed across modern machine learning. These axioms aim not to model a specific architecture, but to capture the universal regularities of systems that learn under resource constraints.

Ultimately, this program treats intelligence as a phenomenon of structure formation—the spontaneous emergence of invariance and capability from scaling and interaction. Methodologically, it follows a path similar in spirit to Shannon’s Mathematical Theory of Communication: beginning from empirical regularities in an experimental domain and distilling them into a minimal set of axioms that yield predictive laws. In the same way that Shannon transformed communication into an exact mathematical discipline, this work seeks to transform learning itself into one—replacing heuristic observation with structure, invariance, and proof. Axiomatic learning theory thus aspires to serve as the mathematical backbone of artificial intelligence: a foundation explaining not only how intelligent systems are built, but why they must emerge.
\end{abstract}

\noindent\textbf{Keywords:} Emergence, Scaling Laws, Learning Theory, Axiomatic Systems.

% === Table of Contents ===
\tableofcontents

\section*{Notation}
\phantomsection
\addcontentsline{toc}{section}{Notation}

The following symbols are used throughout the paper.  
Calligraphic letters denote abstract spaces; capital Roman letters denote elements of those spaces; Greek letters denote functions, kernels, and distributions.

\vspace{1em}

\subsection*{Spaces and Elements}

\begin{tabular}{ll}
$\mathcal D$ & Space of data domains \\
$\Omega\in\mathcal D$ & Data domain \\
$\mathcal X$ & Input space \\
$\mathcal Y$ & Output space \\
$\mathcal P$ & Procedure space \\
$P\in\mathcal P$ & Procedure \\
$\mathcal S = \mathcal D\times\mathcal P$ & Learning space \\
$S\in\mathcal S$ & Learning system \\
$\mathcal R$ & Resource base \\
$R\in\mathcal R$ & Resource vector \\
$\Theta$ & Parameter space \\
$\theta\in\Theta$ & Trained parameter state \\
\end{tabular}

\vspace{1em}

\subsection*{Probability and Training Objects}

\begin{tabular}{ll}
$\mu$ & Data-generating distribution \\
$K(S,R)$ & Training kernel (Markov kernel) \\
$\Pi_\theta$ & Predictor map (stochastic kernel) \\
$\Prob(\Theta)$ & Space of probability measures on parameters \\
$\gamma$ & Coupling of probability measures \\
$\Rightarrow$ & Weak convergence of measures \\
\end{tabular}

\vspace{1em}

\subsection*{Capability Objects}

\begin{tabular}{ll}
$\Phi = (\mathcal T,\mathcal U,\mathcal V_\Phi)$ & Capability type \\
$\mathcal T = (\mathcal X,\mathcal Y,\mu)$ & Learning task \\
$\mathcal U$ & Learning utility \\
$\mathcal V_\Phi$ & Capability codomain \\
$M_\Phi(\theta)$ & State-level capability score \\
$m_\Phi(S,R)$ & Seed-averaged (expected) capability \\
$\tau_\Phi(S,R)$ & Representability threshold \\
$\Sigma_c(S,\Phi)$ & Iso-capability set $\{R: m_\Phi(S,R)=c\}$ \\
\end{tabular}

\vspace{1em}

\subsection*{Resource Geometry and Scaling}

\begin{tabular}{ll}
$\psi:\mathcal R\to\mathbb R^k$ & Feature map of the resource base \\
$\alpha\in(\mathbb R^k)^*$ & Effective direction \\
$s(R)=\alpha^\top\psi(R)$ & Effective resource scalar \\
$f:\mathbb R\to\mathbb R$ & Scaling function (S-curve) \\
\end{tabular}

\vspace{1em}

\subsection*{Order and Topological Structure}

\begin{tabular}{ll}
$\preceq$ & Resource order \\
$\oplus$ & Resource composition (monoid operation) \\
$\tau_\mathcal R$ & Resource topology \\
$\tau_\mathcal S$ & Learning-system topology \\
\end{tabular}

% === Acknowledgements ===
\section*{Acknowledgements}
\phantomsection
\addcontentsline{toc}{chapter}{Acknowledgements}
I thank...

% === Main Matter ===

\section{Introduction}

\subsection{The Unexplained Emergence of Intelligence}
Over the past decade, modern machine learning has entered an era of striking empirical regularities. When models are scaled in data, parameters, and compute, their performance follows smooth power-law trends across orders of magnitude \citet{hestness2017deep, kaplan2020scaling, hoffmann2022training}. Yet amid these predictable curves, a different kind of phenomenon appears: qualitatively new capabilities emerge abruptly. Large language models, for example, acquire reasoning, coding, or translation abilities not present in smaller versions \citet{wei2022emergent, srivastava2022beyond}. These transitions suggest that scaling does more than enlarge existing capacities—it reorganizes the effective structure of learning itself.

Despite their prevalence, such emergent effects remain mathematically unexplained. Current theories describe optimization, generalization, and expressivity, but not why a continuous change in scale can produce discontinuous jumps in capability. Empirical scaling laws quantify the behavior of loss surfaces, yet they do not identify the invariant structures underlying those laws. The field thus stands in a situation reminiscent of information theory before Shannon: an abundance of experimental facts, no formal vocabulary to express them.

This paper pursues the hypothesis that emergence in learning systems possesses an underlying order amenable to mathematical description. Rather than treating emergence as an incidental property of particular architectures, we seek to express it through minimal structural assumptions that hold across architectures and tasks. In doing so, we follow the methodological spirit of the Erlangen and Shannon programs. Just as Klein proposed that geometry be characterized by invariance under transformations \citet{bronstein2021geometric}, and Shannon formalized communication by a handful of axioms \citet{shannon1948mathematical}, we ask whether the scaling behavior of learning systems can be organized by a comparable set of first principles.

Such an endeavor requires bridging two perspectives: the empirical regularities revealed by large-scale experiments, and the abstract mathematical language capable of encoding them. If successful, an axiomatization of emergence would allow us to move from phenomenological observation to deductive prediction—clarifying when and why new capabilities must appear as resources grow. It would provide, in short, a conceptual grammar for the unexplained order observed in modern scaling phenomena.

\subsection{Lack of Formal Foundations}
Despite remarkable empirical progress, contemporary deep learning theory lacks a mathematical language in which the phenomenon of emergence can even be posed precisely. Existing theories describe certain facets of learning—optimization dynamics, expressivity of architectures, and generalization under fixed capacity—but none provide a substrate where scaling itself can be treated as a formal variable and capability as a measurable function over it.

The standard approaches treat a model as a parametrized function class and study convergence properties as sample size increases. However, these analyses presuppose a static notion of the “learning system.” When scale changes, we do not merely re-train a fixed system with more data; we instantiate a different system, defined by a new architecture, training regime, and parameterization. The relevant space of objects therefore is not a single model, but a population of learning systems evolving under resource constraints. Within current theory, no canonical structure connects these systems to one another. Consequently, concepts like emergent capability—a property observed only across scales—lack a well-defined mathematical representation.

Moreover, our existing theoretical frameworks operate at different levels of abstraction, but none bridge the empirical and the formal. Statistical learning theory provides asymptotic bounds on risk but is insensitive to architectural scale. Scaling-law analyses \citet{kaplan2020scaling, hoffmann2022training} reveal quantitative regularities but lack the notion of topology or continuity that would allow one to speak of phase transitions in capability. Geometric Deep Learning \citet{bronstein2021geometric} introduced a unifying geometric view of inductive bias, yet it treats the underlying group symmetries as given rather than emergent. What is missing is a framework that unifies these views: one capable of expressing learning systems, resources, and capabilities as formal objects within a shared mathematical space.

Without such a substrate, our understanding of emergence remains descriptive rather than explanatory. We can observe scaling curves and capability thresholds, but we cannot derive them or determine which observed patterns are fundamental and which are contingent. A theory of learning that aspires to generality must therefore begin at a deeper level: by defining the minimal structural conditions under which a system of learning systems—when scaled—must display emergent behavior.

\subsection{Towards an Axiomatic Theory of Learning Systems}
The objective of this work is to construct a mathematical framework that explains emergent capabilities in learning systems through a small set of falsifiable axioms. The goal is not to model any particular architecture or algorithm, but to identify the minimal structural conditions under which qualitative changes in capability necessarily arise as a function of scale.

We begin from a general guiding question: \textit{What are the smallest mathematical conditions under which the experimental phenomena of deep learning must occur?}. More specifically, \textit{under what general conditions does a learning system acquire new capabilities as its resources increase?}.

To answer it, we adopt the methodological stance of axiomatic science. Rather than accumulating descriptive models, we seek to isolate a handful of postulates—expressed in the language of topology, measure theory, and probability—that make emergence a derivable property of the system. This is akin to what Shannon accomplished for communication theory \citet{shannon1948mathematical} and what Klein’s Erlangen program achieved for geometry \citet{bronstein2021geometric}: the transformation of an empirical field into a formal one by identifying its invariants.

The framework pursued here treats a learning system not as an isolated model but as a point in a continuous mathematical space—a product of data-generating processes and construction recipes. Scaling then corresponds to trajectories through this space, parameterized by resource vectors (data, parameters, compute). Within this setting, a capability becomes a bounded functional $m_\Phi(S,R)$ that maps a system–resource pair to an expected performance score. Emergence can then be defined precisely as a qualitative transition in this functional’s structure—such as the sudden appearance of an open region where a capability becomes representable.

An axiomatic approach to emergence serves two purposes. First, it provides a unifying theoretical language that can generalize across architectures, tasks, and modalities. Second, it renders “emergence” empirically testable: each axiom can be confirmed or falsified through experiment. The resulting theory thus aims not for metaphysical explanation but for predictive compression: a minimal set of assumptions from which scaling laws, critical thresholds, and universality curves follow as theorems rather than observations.

If successful, such a framework would stand to deep learning as thermodynamics once stood to statistical physics—a higher-level, invariant description of phenomena that remain stable under the details of implementation. It would transform the mystery of emergent behavior from a narrative about surprising model behavior into a structured mathematical question: what properties of the learning system’s space force emergence to occur?

\subsection{Structural Formalization of Emergent Capabilities}
To move from intuition to formalism, we must identify the mathematical objects that constitute the domain of emergence. A theory of learning systems cannot rely on empirical definitions—such as architectures, datasets, or optimizers—whose meanings shift across experiments. Instead, it must abstract these into entities that admit continuity, convergence, and invariance: the basic ingredients of any axiomatic science.

The central insight guiding this work is that a learning system can be treated as a point in a structured space. We begin by defining the data domain $\Omega$ (the statistical source of observations) and a procedure $P$ (the specification of how models are built and trained). Each carries a natural topology: the weak topology on distributions for data, and a product topology on hyperparameters and algorithms for recipes. Their product forms the learning space
\[
\mathcal S=(\Omega, \tau_\Omega) \times (\mathcal P , \tau_\mathcal P),
\]
so that a specific learning system is represented as a point $S=(\Omega, P)$ in this topological space. This abstraction allows us to speak rigorously about neighborhoods of similar systems, continuity of transformations between them, and convergence under scaling.

Scaling itself is expressed through a resource vector
 \[
 R\in \R^d_{>0},
 \]
whose coordinates correspond to tokens, parameters, compute, or other measurable resources. These live in a Euclidean topology and are often studied through smooth feature maps $\psi(R)=(\log R_1,\dots, \log R_d)$ that linearize multiplicative scaling effects. Thus, changing a model’s scale corresponds to following a path through resource space.

Training introduces stochasticity, which we represent as a Markov kernel
\[
K:\mathcal S \times \R^d_{>0} \to \mathcal P (\Theta),
\]
mapping each system–resource pair to a probability distribution over trained states $\theta\in\Theta$. This captures the randomness of initialization, data shuffling, and noise during optimization, ensuring that the theory reasons about distributions of outcomes rather than single training runs.

Each trained state $\theta$ induces a predictor map
\[
\Pi_\theta :\mathcal X \to \mathcal P(\mathcal Y),
\]

assigning to every input a distribution over outputs—encapsulating the model’s belief structure. A capability is then defined as a task–utility pair $\Phi= (T_\Phi, U_\Phi)$ that measures how effectively these predictions align with ground truth. Averaging over the stochasticity of training yields the seed-averaged capability

\[
m_\Phi(S,R)=\E _{\theta\sim K(S,R)} [M_\Phi(\theta)],
\]
which serves as the order parameter of the theory: a continuous function over system and resource space quantifying performance under scaling.

Within this formalism, the role of the forthcoming axioms is to specify invariant relations that $m_\Phi(S,R)$ must satisfy—continuity, monotonicity, universality, and representability—under broad structural conditions. These axioms mirror the regularities observed empirically in large models but express them in a coordinate-free, topological language.

Conceptually, this construction parallels the unifying philosophy of Geometric Deep Learning \citep{bronstein2021geometric}: where GDL describes inductive bias through invariance under symmetry groups, we describe emergence through invariance under scaling transformations. The difference is one of ontology: GDL assumes a given group action on data; this framework asks what minimal structure forces such order to appear spontaneously.

The resulting formal setting—topological learning spaces, resource manifolds, stochastic training kernels, and continuous capability maps—constitutes the mathematical substrate upon which the axioms of emergence are stated. It provides the precise language in which empirical phenomena like “sharp transitions” and “universality curves” can be reformulated as theorems.

\subsection{Structure and Validation}
The remainder of this work is organized to translate the conceptual program above into a coherent mathematical and empirical framework.

In Chapter 2, The Domain of Emergence Theory, we formalize the mathematical objects on which the theory rests. We define learning systems as points in a product topological space of the data domain and a "procedure", formalize resources as positive vectors controlling scale, and describe the stochastic nature of training through a Markov kernel mapping systems and resources to distributions over trained states. This yields the central functional $m_\Phi(S,R)$—the seed-averaged capability—that serves as the quantitative order parameter of emergence.

In Chapter 3, Axiomatizing Emergence, we introduce the axioms governing this functional. The first layer of structural axioms ensures that the theory is well-posed: the necessary topological and measure-theoretic regularity conditions under which continuity, measurability, and convergence hold. Upon this substrate, the axioms of emergence (A1–A6) specify the invariant relations that empirical scaling phenomena appear to obey—representability, universality, monotonicity, and local continuity—culminating in a one-dimensional representation theorem that expresses emergent capability as a universal scaling curve.

Chapter 4 develops the theoretical consequences of these axioms, establishing that under the stated conditions, any continuous measure of capability must locally collapse onto a single effective resource axis. This result links the formal structure of the theory to the empirical scaling laws observed in large models.

Finally, Chapters 5–6 test the framework empirically. Rather than treating experiments as mere illustrations, they are designed as falsification attempts—systematic efforts to locate the boundaries where the axioms fail. Each experiment, from toy tasks such as modular addition and parity to larger synthetic setups, probes a different aspect of the axioms: monotonicity, continuity, universality, and representability. This approach reflects the central epistemic stance of the theory: that mathematical structure gains meaning only through its ability to withstand empirical refutation.

The overall aim is therefore twofold. First, to propose a unified formal substrate for studying capability emergence in learning systems—independent of architecture, modality, or optimization details. Second, to establish a research program in which scaling phenomena become not just observations but mathematical consequences of a small, testable set of principles. By grounding the study of emergence in falsifiable structure, this work seeks to transform a descriptive mystery of modern AI into a mathematically rigorous, predictive science.

\section{The Domain of Emergence Theory}

To construct a mathematical theory, one must first delineate its domain of discourse—that is, the class of objects the theory concerns.

The purpose of this chapter is to define these foundational objects and relations, establishing a precise mathematical setting in which the phenomena of emergence can be studied. This includes specifying what constitutes a learning system, what resources are subject to scaling, and how we quantify emergent capability.

The formal ingredients introduced here are conceptually parallel to those used in \textit{Geometric Deep Learning} \citet{bronstein2021geometric}: domains, symmetries, signals, and mappings. Yet our formulation departs from specific architectures, framing these ingredients in a level of abstraction sufficient to encompass \textit{any} learning system.

This generality is essential. Although the motivating examples arise from neural networks, the goal is not to describe particular architectures but to identify structural invariants—mathematical properties that remain stable across scales, models, and modalities.

At the center of this construction lies the notion of a learning system. To analyze emergence rigorously, we must transcend implementation details and instead define an abstract representation that balances expressive power with mathematical tractability.

\subsection{The Data Domain and Value Space}

\begin{definition}[The Data Domain]\label{def:data-domain}
    The \textit{data domain} $(\Omega, \tau_\Omega)$ is a topological space representing the set of possible inputs on which signals are defined. When equipped with a Borel $\sigma$-algebra $\mathscr B(\Omega)$ (\ref{app:borel}), it becomes a measurable space $(\Omega, \mathscr B(\Omega))$ (\ref{app:measurable-space}).
\end{definition}

$\Omega$ formalizes the "world" of observations. Each $\omega\in\Omega$ represents a point, node, or token. It plays the same role in GDL \citet{bronstein2021geometric} where signals live. 

\begin{definition}{The Space of Data Domains}{}
    It will occasionally be useful to talk about the \textit{space of data domains}, denoted $\mathcal D$. 
\end{definition}

\begin{definition}[Value Space]\label{def:value-space}
    The \textit{value space} $(\mathcal C, \tau_\mathcal C)$ is a topological vector space (\ref{app:tvs}). When equipped with a Borel $\sigma$-algebra $\mathscr B(\mathcal C)$ (\ref{app:borel}), it becomes a measurable space $(\mathcal C, \mathscr B(\mathcal C))$ (\ref{app:measurable-space}).
\end{definition}

The value space $\mathcal C$ is an arbitrary topological vector space (often finite-dimensional, but may be non-linear, discrete, or manifold-valued). It encodes what kind of quantity the signal represents, which could be intensities, labels, features, probability distributions, etc.

In \citet{bronstein2021geometric}, $\mathcal C$ is called the \textit{channel} or \textit{feature} space. Here, the word \textit{value} space is used since it matches the level of abstraction we are after.

\subsection{Signals and The Signal Space}

\begin{definition}[Signals]\label{def:signals}
    A \textit{signal} is a measurable map (\ref{app:measurable-map})
    \[
    x:\Omega\to \mathcal C
    \]
    mapping each element of a domain $\Omega$ to an element of a \textit{value space} $\mathcal C$.
\end{definition}

Intuitively, all we are saying is that $x$ takes elements of "data domain" $\Omega$ (to be defined) and maps them to elements of the value space $\mathcal C$.

Machine learning is about estimating functions. The set-up is that we have some data consisting of $n$ samples/observations $\{(x_i,y_i)\}^N _{i=1}$ independent and identically distributed from the data distribution defined over the product space of the input- and output domain $\mathcal X\times \mathcal Y$

\begin{definition}{The Signal Space}{}
    Given $\Omega$ and $\mathcal C$, define the space of signals
    \[
    \mathcal X(\Omega,\mathcal C)=\{ x:\Omega\to \mathcal C \mid x \text{ is measurable and } \|x\|<\infty \}
    \]
\end{definition}

As noted in \citet{bronstein2021geometric}, if $\mathcal C = \R^n$ and $\mu$ is finite, this is the Hilbert space $L^2 (\Omega, \mu ;\R^n)$ with inner product defined as:
\[
\langle x,y\rangle = \int_\Omega \langle x ( \omega ) , y(\omega)\rangle_\mathcal C\ d\mu ( \omega)
\]

Intuitively, $\mathcal X $ is the formal object we will later apply operators to. It's where training and inference act.

\subsection{Symmetries and Group Actions}

To talk about invariants and inductive bias we need to formally define the notion of symmetry. 

\begin{definition}{Symmetry Group}{}
    A symmetry group (\ref{app:groups}) $G$ acts on $\Omega$ via homeomorphisms $g:\Omega\to \Omega$ preserving structure:
    \[
    (g\cdot x)(\omega)=x(g^{-1} \omega)
    \]
    $G$ encodes equivalence transformations of the domain
\end{definition}

This will be of great importance when questioning whether such a $G$ must exist for all learning systems.
 
\subsection{Procedure and Learning Systems}

The problem with a specific definition of a learning system is that it is not generalizable; deep learning systems vary a lot and much of the field is experimental so naturally a specific model would just be a tiny fraction of possible models. This is the motivation for defining a more abstract and general framework for mathematically reasoning about AI models, the cornerstone of which is the definition of a learning system $S$.

\begin{definition}[Procedure Space]\label{def:procedures}
The procedure space $(P,\tau_P)$ is a Hausdorff topological space whose elements 
encode model architectures, optimization hyperparameters, parameter initializations, 
and other controllable training choices.
\end{definition}

\begin{definition}[Learning System]\label{def:learning-system}
A learning system is a pair $S = (D,P)$ consisting of a data domain $D$ and a 
procedure $P$. The space of learning systems is the product topological space
\[
\mathcal S = \Omega \times P, \qquad \tau_\mathcal S = \tau_\Omega \times \tau_P.
\]
\end{definition}

This asserts that both domains (data) and procedures (algorithms, architectures, optimizers, etc.) have a notion of closeness or continuity, and that the joint space of systems inherits a product topology. It’s the foundation for all later continuity and measurability statements.

It’s empirically neutral and doesn’t assert any scaling behavior or monotonicity.

Intuitively, all we are saying is that the different ways we might create our end-model all live in the same abstract topological space $\mathcal P$. Moreover, in the case of modern deep learning, a procedure $P$ is usually itself a longer tuple with each component equipped with its own natural topology, making $\mathcal P$ into a topological product space (\ref{app:product-topologies}). 

The procedure simply includes information about the way intelligent models are constructed.

When we later say that “a capability emerges in $S$” we must be referring to a particular class of systems—a particular class of $S$. It is only then we can ask questions like “how does this behavior generalize over multiple $S$?” which will be a very important wonders to explore.

With topology,  the idea of “two systems are similar” becomes precise via neighborhoods and metrics. Topology could yield the formal language for talking about sequences of models, limits, continuity, convergence, and so on. 

Before we move on, we dedicate the next section to exemplify this fundamental abstraction. I will take our specific learning system we defined earlier, and define the topologies for each each space. This will get a bit theoretical but will hopefully strengthen the validity of this axiom.

\subsubsection{Philosophical Note on the Topologization of AI Models}

At first glance, representing a learning system as a tuple 
\[
S=(\Omega, P)
\]
where the procedure $P$ contains objects such as architectures, optimizers, losses, and initialization schemes, may appear unconventional. Treating such a tuple as a point in a topological space forces us to ask: what kind of mathematical object is this?

This abstraction, however, mirrors the foundational moves in several mature fields. In probability theory, a “random variable” is not a variable but a measurable mapping (\ref{app:}). In category theory, a “category” is a highly structured tuple treated as a point in Cat (\ref{app:}). In geometric deep learning, a graph neural network is treated as a point in a space of equivariant signal processors (\ref{app:}). In statistical mechanics, a spin configuration is a giant tuple in a product topology (\ref{app:}).

Throughout history, complex, heterogeneous objects gain mathematical coherence once embedded into a topological or measurable space. 

Here, the learning system is not a specific model but a design specification. Once placed inside a topological structure, continuity of training, representability, and scaling become well-defined as we ought see. This abstraction is precisely what allows the theory to state and prove general laws of emergence that remain valid across architectures, optimizers, and data modalities.

\subsubsection{Example of a Learning System—The Canonical Learning Space Topology}

Going back to our old example where $P=(\mathcal L, A, \rho, \iota, F)$, I will now propose some natural topologies for their respective topological space. In this example:

\begin{itemize}
    \item $(\mathcal L, \tau_{\mathcal L})$: per-example loss space,
    \item $(A, \tau_A)$: training algorithms,
    \item $(\rho, \tau_\rho)$: regularization schemes,
    \item $(\iota, \tau_\iota)$: initialization distributions,
    \item $(F, \tau_F)$: architecture families,
\end{itemize}
so:
\[
\mathcal S = (\Omega, \tau_\Omega)\times (\mathcal L, \tau_{\mathcal L}) \times (A,\tau_A) \times (\rho,\tau_\rho) \times (\iota,\tau_\iota) \times (F,\tau_F).
\]

Suppose $(X,d)$ is a metric space (e.g. sequences, tokens, or $\mathbb R^n$). Let $\mathcal P(X)$ denote the set of Borel probability measures on $X$. To treat data-generating processes as mathematical objects, we must equip $\mathcal P(X)$ with a topology, i.e. a notion of convergence of distributions.

\textbf{Topology $\tau_D$:} There are several possible notions of convergence for measures. In our setting, the natural choice is the **weak topology** $\tau_D$, under which a sequence $(D_n)$converges to $D$ if and only if

\[
D_n \Rightarrow D 
 \iff 
\int f  dD_n \longrightarrow \int f  dD
\quad \text{for all } f \in C_b(X),
\]

where $C_b(X)$ denotes the space of bounded continuous functions on $X$.

This topology is standard in probability theory and stats. It expresses the idea that probability measures are close whenever they yield similar expectations for all continuous observables. In particular, weak convergence formalizes the intuition that empirical distributions converge to the true data distribution as sample size grows.

$\mathcal L$ is the set of per-example losses, which we treat as measurable functions $\ell: Y \times \hat{Y} \to \mathbb R_{\geq 0}$.

\textbf{Topology $\tau_\mathcal L$:} Uniform convergence on compact sets, or the $L^p$-topology with respect to the data distribution $D$.

Losses are functionals. Their behavior is stable if they converge uniformly. This makes the risk functional $\mathbb E_D[\ell]$ continuous.

$A$ is the set of algorithms mapping data with parameters and output updated parameters.

We can think of each algorithm as a map $T: \Theta \times \Omega \to \Theta$, where $\Theta$ is the parameter space.

\textbf{Topology $\tau_A$:} compact-open topology (standard for function spaces), or equivalently pointwise convergence on compacts.

It ensures “if two optimizers are close, then their parameter updates are close on compact sets of inputs.”

$\rho$ are maps or functionals modifying training dynamics (e.g. penalties, dropout distributions, augmentations).

These are again functions/transformations.

\textbf{Topology $\tau_\rho$:} operator norm topology if regularization is linear, or compact-open topology if nonlinear.

It ensures stability under small changes in the regularization scheme.

$\iota$ is a probability measure on the parameter space $\Theta$.

\textbf{Topology $\tau_\iota$:} weak topology on probability measures (same as for data $D$).

Initialization schemes are compared by the limiting distribution of parameter samples.

$F$ is the set of model architectures, thought of as families of functions $f_\theta:  X \to Y$parameterized by $\theta \in \Theta$.

\textbf{Topology $\tau_F$:} Pointwise convergence of functions ($f_n \to f$ if $f_n(x)\to f(x)$ for all $x$). Or, stronger, uniform convergence on compact subsets of $X$.

Architectures are function classes; topologies of function spaces are standard here. Uniform-on-compacts is natural for deep learning since inputs are usually bounded sequences.

\subsection{Resources}

Our central question revolves around explaining the general conditions under which a
learning system acquires new capabilities as resources increase. Thus, the construction 
of "resources" in this theory is of great importance.

\subsubsection{The Resource Base}

In studying emergence, we need a space that captures how much of each measurable 
quantity a learning system consumes or possesses (data, compute, parameters, 
optimization steps, etc.). This space must let us:
\begin{enumerate}
    \item compare two configurations (“does system A have more resources than B?”),
    \item combine resources when they accumulate (“training with both datasets is like adding their sizes”), and
    \item vary resources continuously to study scaling limits and local neighborhoods.
\end{enumerate}

These requirements naturally lead to an algebraic–topological structure with an order 
$\preceq$ to express "no less than," a binary operation $\oplus$ to express accumulation, 
an identity element $0$ representing “no resource,” and a topology $\tau_\mathcal R$ to 
express smooth change and convergence. Together these define what we call the resource base.

Elements of $\mathcal R$ represent specific configurations of measurable resources 
allocated to a learning system (e.g. training tokens, model parameters, or optimization 
steps). We write $R\in \mathcal R$.

We require an operation $\oplus:\mathcal R \times \mathcal R \to \mathcal R$ that 
combines two resource allocations into a joint one:
\[
(R_1,R_2)\mapsto R_3.
\]
Intuitively, $R_1\oplus R_2=R_3$ just means that we are using $R_1$ and $R_2$ together 
as $R_3$. 

\begin{definition}[Resource Base]\label{def:resource-base}
The resource base is a topological ordered commutative monoid
\[
(\mathcal R,\oplus,0,\preceq,\tau_\mathcal R)
\]
satisfying:
\begin{enumerate}
    \item $(\mathcal R,\oplus,0)$ is a commutative monoid,
    \item $(\mathcal R,\preceq)$ is a closed partial order,
    \item $\oplus$ is monotone in each argument with respect to $\preceq$,
    \item $\tau_\mathcal R$ is Hausdorff and makes $\oplus$ and the order relation continuous.
\end{enumerate}
\end{definition}

\begin{definition}[Resource Order]\label{def:resource-order}
    A \textit{resource order} is a partial order $\preceq$ on $\mathcal R$ compatible 
    with $\oplus$, meaning that
    \[
    R_1 \preceq R_2 \ \Rightarrow\ R_1 \oplus R_3 \preceq R_2 \oplus R_3,
    \]
    for all $R_1,R_2,R_3\in\mathcal R$.
\end{definition}

"$R_1\preceq R_2$" reads intuitively as "$R_2$ provides at least as much of every 
resource as $R_1$."

\begin{definition}[Resource Topology]\label{def:resource-topology}
    A \textit{resource topology} $\tau_\mathcal R$ on $\mathcal R$ is a Hausdorff 
    topology making $\oplus$ continuous:
    \[
    (R_1,R_2)\mapsto R_1\oplus R_2
    \quad\text{is continuous on}\quad
    (\mathcal R\times\mathcal R,\tau_\mathcal R\times\tau_\mathcal R).
    \]
\end{definition}

The topology encodes smooth variation of resources and enables defining limits, 
continuity, and neighborhoods in $\mathcal R$.

A resource base is to learning systems what a geometric domain is to signals: an 
abstract set where composition, comparison, and continuity are defined. The formalism 
requires only that resources can be composed, compared, and varied continuously. The 
Euclidean model $\R^d_{>0}$ serves as a concrete instantiation but is not assumed 
axiomatically.

In practical machine learning, a training run can be characterized by measurable 
quantities such as tokens processed, parameter count, optimizer steps, FLOP compute, 
and dataset size. Let us denote such quantities collectively by the vector
\[
R=(R_\text{data}, R_\text{params}, R_\text{steps}, R_\text{compute},\dots)\in \R^d_{>0}.
\]
Then the tuple 
\[
(\mathcal R, \oplus, 0 ,\preceq , \tau_\mathcal R)
    =(\R^d_{>0},+,0,\le,\tau_\text{Euclidean})
\]
forms a resource base. The operation $R_1\oplus R_2=R_1+R_2$ corresponds to combining 
independent training runs or budgets, the order $R_1\preceq R_2$ holds when every 
resource component of $R_1$ does not exceed that of $R_2$, and the topology is the 
standard Euclidean one, ensuring that small changes of resources produce continuous 
changes in system behavior. 

Under this model, a sequence $(R_n)$ with $R_n\to R_\infty$ captures the notion of 
"scaling up" a training configuration continuously, and the study of emergent 
capabilities can be expressed as limits of expected capability (to be defined) over 
such sequences or neighborhoods in $\mathcal R$.

\subsubsection{Feature Maps And Resource Directions}

Before we define what we mean by a feature map $\psi$ and a "direction" $\alpha$ in a resource base, we clarify each conceptually: The feature map $\psi$ provides \textit{coordinates} for resources in the relevant resource base. It is how we turn abstract elements $R\in \mathcal R$ into numerical representations that can enter analytic expressions. It is simply a convenient embedding into a familiar space where one can do geometry and analysis. The "direction" $\alpha$ then specifies which "axis" of this coordinate system corresponds to \textit{effective scaling}, i.e., the dimension along which capability transitions occur. Formally, it's not necessarily a Euclidean vector, but an element of the dual space to wherever $\psi$ maps.


\begin{definition}[Feature Map]\label{def:feature-map}
    Let $(\mathcal R, \oplus, 0, \preceq, \tau_\mathcal R)$ be a resource base.  
    A \textit{feature map} is a continuous, injective monoid homomorphism
    \[
    \psi:\mathcal R \to \mathbb R^k
    \]
    satisfying:
    \begin{enumerate}
        \item $\psi(0)=0$,
        \item $\psi(R_1\oplus R_2)=\psi(R_1)+\psi(R_2)$,
        \item $R_1\preceq R_2 \iff \psi(R_1)\le \psi(R_2)$ coordinate-wise.
    \end{enumerate}
    The image $\psi(\mathcal R)\subseteq\mathbb R^k$ defines a concrete Euclidean realization of the abstract resource base.
\end{definition}

Intuitively, $\psi$ embeds the abstract resource base structure into Euclidean coordinates that respect combination, ordering, and continuity.

In the canonical case $\mathcal R = \R^d_{>0}$, $\psi(R)=\log R$ is a typical choice, mapping multiplicative scaling to additive coordinates.

\begin{definition}{The Effective Direction}
    Given a feautre map $\psi:\mathcal R \to \R^k$, an \textit{effective direction} $\alpha$ is a nonzero element 
    \[
    \alpha\in (\R^k)^*
    \]
    of the dual space, representing a functional that projects resource embeddings onto an \textit{effective-resource axis}:
    \[
    s(R):= \alpha(\psi(r))=\alpha^\top \psi(R)
    \]
\end{definition}

Only the direction of $\alpha$ matters, not its magnitude; scalar multiples are considered equivalent. Intuitively, $\alpha$ defines how the combination of tokens, parameters, and compute collapse into a single "effective scale" variable--the axis along which emergence curves are aligned.

\subsubsection{Resource Equivalence}

\begin{definition}[Capability Equivalence of Resources]\label{def:cap-equiv}
For a fixed state $S$, define an equivalence relation on resources by
\[
    R \sim R'
    \quad\Longleftrightarrow\quad
    m_\Phi(S,R)=m_\Phi(S,R').
\]
The equivalence classes of $\sim_{\rm cap}$ are the iso-capability sets.
\end{definition}

\subsection{The procedure-Local Neighborhood}

We want to define a metric space (or at least a topological space) of learning systems, so that when we later talk about continuity of the training kernel, predictor map, and expected capability, it all has rigorous meaning. The objects of this space are, as defined, $S=(\Omega,P)$ where $\Omega\in \mathcal D $ is a data domain and $P\in \mathcal P$ is a procedure. The learning space is the product space $\mathcal S =\mathcal D \times \mathcal P$, and to give it a topology, we define two metrics: The data distance $d_\mathcal D$ on $\mathcal D$ and the procedure distance $d_\mathcal P$ on $\mathcal P$. Then we can form the combined neighborhood basis:
\[
U(S;\epsilon_\mathcal D, \epsilon_\mathcal P ) = B_{d_\mathcal D}(\Omega; \epsilon_D) \times B_{d_\mathcal P}(P;\epsilon_\mathcal P)
\]

\subsubsection{The Data Distance}

Following \citet{bronstein2021geometric}, we import the definition of a data-domain distance. Their framework provides a mathematically grounded way to measure similarity between data domains that may have very different underlying structures—graphs, manifolds, or Euclidean grids—through the geometry of their metric spaces. This notion of distance is essential for our purposes since it gives a rigorous meaning to statements such as “two learning systems are trained on similar domains,” and it allows us to define neighborhoods, continuity, and convergence in the learning-space topology introduced later.

\begin{definition}{The Data Distance}{}
Let $\mathcal D$ be the set of admissible data domains. For $\Omega,\Omega'\in \mathcal D$, define:
\[
d_\mathcal D(\Omega, \Omega' ):= \inf_{\eta\in G} \|d_\Omega - d_{\Omega'} \circ ( \eta\times \eta) \|,
\]
where $G\subseteq \text{Bij}(\Omega,\Omega')$ is a group of admissible transformations and $\|\cdot \|$ is an appropriate functional norm.
\end{definition}

The concrete form of $d_\mathcal D$ depends on the nature of the domains in $\mathcal D$:
\begin{itemize}
    \item \textbf{Graphs:} When each domain $\Omega$ is a graph $G=(V,E)$, a natural metric is the graph edit distance, defined as the minimum number of edge or vertex modifications needed to transform $G$ into $G'$. This captures structural similarity between relational datasets.
    
    \item \textbf{Manifolds:} When domains are smooth manifolds endowed with Riemannian metrics $\Omega$, one typically uses the Gromov–Hausdorff distance to compare them, which measures the smallest distortion required to make their pairwise point-to-point distances coincide under some correspondence.
    
    \item \textbf{Euclidean datasets:} For domains embedded in $\R^n$ (e.g., text token sequences or image grids), a convenient choice is the Wasserstein distance between the corresponding empirical measures—an optimal-transport formulation that quantifies how much “mass” must move to morph one data distribution into another.
\end{itemize}

\subsubsection{The Procedure Distance}

Unlike the data domain $\Omega$, the space of procedures $\mathcal P$ (training algorithms, optimization schedules, hyperparameters, etc.) lacks a single canonical geometry. Some procedure attributes like optimizer type or normalization layers are categorical: they identify qualitatively distinct regimes that cannot be continuously deformed into another. Other attributes like learning rate, weight decay, dropout rate, or initialization scale are continuous and admit smooth variation. 

To define a meaningful notion of "closeness" between two procedures, we therefore decompose each procedure $P\in \mathcal P$ into a categorical component $c(P)\in C$ and a continuous hyperparameter vector $h(P)\in \R^p$. The set $C$ is given the discrete topology, reflecting that distinct categorical configurations are separated by finite distance, while $\R^p$ carries the Euclidean topology, allowing infinitesimal perturbations in continuous hyperparameters.

\begin{itemize}
    \item \textbf{Categorical features $c(P)$} (one-hot or simple labels): \begin{itemize}
        \item optimizer family (AdamW, SGD, Lion, …)
        \item schedule shape (cosine, linear, step, …)
        \item architecture family (attn-only, full transformer, conv-mixer, …)
        \item positional encoding type (sinusoidal, RoPE, ALiBi, …)
        \item normalization / activation (LayerNorm vs. RMSNorm; ReLU/GELU/SiLU)
        \item precision regime (bf16, fp8, 8-bit optimizers)
    \end{itemize}
    \item \textbf{Continuous features $h(P)$} (we use logs for positive scales): \begin{itemize}
        \item $\log$ weight decay, dropout rate, label smoothing
        \item $\log $ init variance or fan-in scale
        \item optimizer hyperparams ($\beta_1$, $\beta_2$, $\epsilon$, $\alpha$), gradient clip
        \item schedule shape parameters (e.g., warmup fraction), not LR magnitude if you allow ±10% jitter
    \end{itemize}
\end{itemize}

To capture both continuous and categorical aspects of procedures, we define two complementary: a continuous part $d_\text{cont}$ that measures small perturbations in continuous hyperparameters, and a categorical part $d_\text{cat}$ that separates distinct procedural families into disconnected components. Their combination $d_\mathcal P =d_\text{cont}+d_\text{cat}$ induces the topology $\tau_\mathcal P$ whose basic open sets correspond to tolerance-weighted Euclidean neighborhoods within each categorical class.

\begin{definition}{The procedure Distance}{}

 Let $\mathcal P$ denote the space of procedures, each of which can be represented by a combination of a categorical feature vector $c(P)\in C$ and a continuous hyperparameter vector $h(P)\in \R^p$. Equip $C$ with a metric $d_C :C\times C \to [0,\infty]$ and $\R^p$ with the Euclidean metric weighted by a positive diagonal matrix $W=\text{diag}(1/\tau_1 , \dots,1/\tau_p)$ where each $\tau_j>0$. Define continuous respectively categorical distances as
 \[
    d_\text{cont}(P,P'):= \|W(h(P)-h(P'))\|_2
 \]
 \[
 d_\text{cat}(P,P'):=\begin{cases}
     0,&c(P)=c(P')\\
     \infty , & c(P)\ne c(P')
 \end{cases}
 \]
 Then, the \textit{procedure distance} is defined as
 \[
 d_\mathcal P ( P,P'):= d_\text{cont}(P,P')+d_\text{cat}(P,P')
 \]
\end{definition}

For $\epsilon>0$, the open $\epsilon$-ball centered at $P$ is
\[
B_{d_\mathcal P}(P;\epsilon)=\{P'\in \mathcal P : d_\mathcal P ( P,P')<\epsilon\}
\]
The collection of all such sets forms a neighborhood basis for the procedure topology $\tau_\mathcal P$.

The resulting distance function $d_\mathcal P$ assigns finite values only when two procedures share the same categorical configuration, measuring within-category proximity through a weighted Euclidean norm on $h$. When categories differ, the distance is infinite, corresponding to disconnected components of the space.
This construction mirrors the intuition that small changes to hyperparameters represent minor procedural variations, whereas switching optimizer families or architectures constitutes an entirely new experimental regime.

Intuitively, the weighting matrix $W$ is a metric tensor in disguise, defining how sensitive the distance is to perturbations in each hyperparameter dimension through a set of positive tolerances $\{\tau_j\}^p_{j=1}$. Each \textit{tolerence constant} $\tau_j>0$ specifies the scale of variation in $h_j$ that counts as "small," shaping the geometry of open neighborhoods in the procedure space without altering its topology. The smaller $\tau_j$ is, the more sensitive the metric is to variations in $h_j$ (a tiny deviation will already produce a large distance). The larger $\tau_j$ is, the less sensitive the metric is (even a big numerical change counts as a "small" perturbation.) These various $\tau_j$don’t change the topology (any positive scaling defines the same open sets), but they affect the geometry; the shape of open balls and how we quantify nearness in practice.

\subsubsection{The Learning System Neighborhood}

With the data domain distance and procedure distance defined, the LS neighborhood can be formalized.

\begin{definition}{The Learning System (LS) Neighborhood}{}
    Let $\mathcal S = \mathcal D \times \mathcal P$ be the learning system. For $S=(\Omega,P)\in \mathcal S$, define the basic neighborhood
    \begin{align*}
        U(S;\epsilon_\mathcal D, \epsilon_\mathcal P)&:=\{S'=(\Omega' ,P'):d_\mathcal D(\Omega,\Omega')<\epsilon _\mathcal D, d_\mathcal P ( P,P')<\epsilon_\mathcal P\}\\
        &= B_{d_\mathcal D}(\Omega; \epsilon_D) \times B_{d_\mathcal P}(P;\epsilon_\mathcal P)
    \end{align*}    
\end{definition}

\subsection{The Training Kernel}

Having defined the structure of the learning space and the resource base, we now formalize the process that links them to trained model states. Training deep models is inherently stochastic: shuffling data, random initialization, dropout, augmentation noise. To reason about outcomes rigorously, we need to model training not as a deterministic function but as a stochastic bilinear map $\mathcal S \times \mathcal R\mapsto \text{trained model state}$. So for any given learning system $\mathcal S = (\Omega, P)$ and resource configuration $R\in \mathcal R$, the training process doesn't produce a single deterministic point in the parameter space, but a \textit{distribution} over trained states. That's exactly what a Markov kernel formalizes, where the input is deterministic and the output is a probability measure over possible parameter states. These parameter states live in the parameter space.

\begin{definition}[Parameter Space]\label{def:paramspace}
The parameter space $(\Theta,\tau_\Theta,\mathcal B(\Theta))$ is a Polish space 
equipped with its Borel $\sigma$-algebra.
\end{definition}

We simply view $\Theta$ as the set of all trained states $\theta\in \Theta$ a procedure $P\in \mathcal P$ can can produce.   

\begin{definition}[Training Kernel]\label{def:kernel}
A training kernel is a Markov kernel
\[
K : \mathcal S \times \mathcal R \longrightarrow \mathrm{Prob}(\Theta)
\]
such that:
\begin{enumerate}
    \item for each measurable $A \subseteq \Theta$, the map 
    $(S,R)\mapsto K(S,R)(A)$ is measurable,
    \item $K$ is weakly continuous in $(S,R)$ with respect to 
    the product topology $\tau_\mathcal S\times\tau_\mathcal R$.
\end{enumerate}
\end{definition}


Intuitively, $K(S,R)$ represents the distribution over trained parameter states $\theta\in \Theta$ obtained by training learning system $S$ under resources $R$. It assigns to each $(S,R)$ a probability measure $K(S,R)$ over the parameter space $\Theta$. To talk about the continuity of this map, we must specify a topology on the space of probability measures. The weak topology is the natural choice because it captures what actually matters in learning: Two distributions over parameters are "close" if they give almost the same expectation for all bounded continuous observables.


Training outcomes are never deterministic: even fixing system design and resource budget, two training runs may lead to different states $\theta$. To model this rigorously, we treat training as a Markov kernel, i.e. a map from inputs $(S,R)$ to probability distributions on outcomes. This allows us to:

\begin{enumerate}
    \item Average over randomness when defining expected capability,
    \item Distinguish structural emergence from “lucky seeds,” and
    \item Align with probability theory, where kernels are the natural object for stochastic transformations.
\end{enumerate}

This construction also allows the expected capability $m_\Phi(S,R)$ to be expressed as an integral with respect to this distribution, giving the theory its probabilistic foundation while remaining agnostic to the detailed optimization dynamics. It abstracts away the internal optimization trajectory, representing only its distributional outcome.

In practical experiments, $K$ is approximated empirically by running multiple training seeds with fixed $S$ and $R$, estimating the resulting distribution of performance or model states.

The kernel can equivalently be realized through a measurable map
\[
\text{Train}:\mathcal S \times \mathcal R \times \Omega\longrightarrow \Theta,
\]

defined on an underlying probability space $(\Omega, \mathcal F,  \mathbb P)$ (\ref{app:probability-spaces}) capturing all sources of randomness (initialization, data shuffling, etc.), such that

\[
K(S,R)=\mathbb P \circ \text{Train}(S,R,\omega)^{-1}
\]


\subsection{The Predictor Map}

Once training has produced a parameterized state $\theta\in\Theta$, we must formalize how that state generates predictions. In other, we need a mathematical object that maps inputs to distributions over outputs according to the trained model’s internal parameters. This is what we call the predictor map.

The predictor map is conceptually distinct from the training map. The training map describes how a learning system produces a distribution over parameter states as we vary data, procedures, and resources. The predictor map describes what any particular trained state believes about the world — how it converts inputs to probabilistic outputs.

In practice, the predictor map generalizes the notion of a model’s forward pass: for each $\theta$, it defines a stochastic kernel $\Pi_\theta(x)$ representing the model’s belief distribution over possible outputs for a given input $x$. This abstraction unifies regression, classification, and sequence modeling under a single formalism.

\begin{definition}[Predictor Map]\label{def:predictor}
For each parameter $\theta\in\Theta$, the predictor is a measurable map
\[
\Pi_\theta : X \to \mathrm{Prob}(Y)
\]
which is continuous in $\theta$.
\end{definition}


Intuitively, $\Pi_\theta$ is the predictive distribution of outputs given the model parameters $\theta$ and input $x$.The predictor map expresses a trained model’s behavior as a stochastic function from inputs to output distributions. In the deterministic case, $\Pi_\theta(\cdot \mid x)$ reduces to a Dirac measure centered at $f_\theta(x)$.

\subsection{The Composition of $\Pi_\theta$ and $K$}

Together, the training kernel $K$ and the predictor map $\Pi_\theta$ define a hierarchy of mappings: training randomness first selects a parameter distribution $K(S,R)$, and then each parameter $\theta$ defines its own predictive kernel $\Pi_\theta$. The composition of these two levels yields the expected predictive behavior of the entire learning system under resources $R$.

\begin{definition}
    The \textit{mixture predictor} is defined as
    \[
    \bar \Pi _{S,R}(\cdot \mid x):= \int _\Theta \Pi_\theta(\cdot \mid x)dK(S,R)(\theta).
    \]
\end{definition}

This integral defines a mixture predictor, representing the average predictive distribution across all trained models sampled from the training kernel.

Structurally, this composition realizes a categorical diagram:
\[
\mathcal S \times \mathcal R \xrightarrow{\ K \ } \text{Prob}(\Theta) \xrightarrow{\ \Pi_\theta \ }\text{Prob}(\mathcal Y)
\]

The decomposition of learning into a training map and a predictor map is more than notational convenience — it provides a rigorous way to separate epistemic uncertainty (over parameter states) from aleatoric uncertainty (over outputs given parameters).This separation underlies the later definition of expected capability and ensures that emergence can be studied as a property of measurable compositions, rather than a property of any single trained model instance.

\subsection{What $\Pi_\theta$ looks like in common cases}

\subsubsection{Classification (finite labels)}
\[
\mathcal Y=\{1,\dots,K\} . \\
\Pi_\theta(\cdot\mid x)=\mathrm{softmax}(z_\theta(x))\in\Delta^{K-1}
\]

Examples of $\mathcal U_\Phi$:
\begin{itemize}
    \item probability of the correct label $\Pi_\theta(\{y\}\mid x)$ (bounded, continuous),
    \item clipped/normalized log-prob, or the Brier score mapped to $[0,1]$.
\end{itemize}

\subsubsection{Regression (real-valued)}

\[
\mathcal Y=\mathbb R
\]

Choose a predictive family, e.g. Gaussian

\[
\Pi_\theta(\cdot\mid x)=\mathcal N(\mu_\theta(x),\sigma^2_\theta(x)).
\]

Examples of $\mathcal U_\Phi$:
\begin{itemize}
    \item mass in a tolerance band $\Pi_\theta([y-\varepsilon,y+\varepsilon]\mid x)$ (proper for the induced set-loss, bounded),
    \item a normalized CRPS/log-likelihood mapped into $[0,1]$ (keep it bounded and continuous).
\end{itemize}

\subsubsection{Seq2seq Modeling}

\[
\mathcal Y=\mathcal V^{*} \quad \text{(finite strings)}
\]

$\Pi_\theta(\cdot\mid x)$ is the autoregressive distribution $\prod_t p_\theta(y_t\mid x,y_{<t})$.

Examples of $\mathcal U_\Phi$:
\begin{itemize}
    \item per-token geometric mean probability
    \[
    \exp\!\left(\frac{1}{|y|}\log \Pi_\theta(y\mid x)\right)\in(0,1],
    \]
    \item probability mass of the acceptable set $A(x)$ (if multiple outputs are valid): $\Pi_\theta(A(x)\mid x)$ or its log mapped to $[0,1]$.
\end{itemize}
\subsection{Quantifying Emergence}

Having defined the structure of learning systems, their resources, and the 
stochastic training map, we now introduce the central evaluative concepts of 
the theory: learning tasks, utilities, capability types, and the associated 
capability scores. These will ultimately give rise to the expected capability 
$m_\Phi(S,R)$, the order parameter whose behavior encodes emergence.

\subsubsection{Learning Tasks}

The learning task $\mathcal T$ defines the probabilistic environment in which
a capability is evaluated: which inputs appear, which outputs are considered
correct or high-value, and with what frequencies.

\begin{definition}[Learning Task]\label{def:learning-task}
Let $\mathcal X$ be an input space, $\mathcal Y$ an output space, 
and let $\mu \in \Prob(\mathcal X \times \mathcal Y)$ be a joint data-generating distribution.
A \textit{learning task} is the probability space
\[
\mathcal T := (\mathcal X, \mathcal Y, \mu),
\]
where $\mu$ defines the statistical relationship between inputs and outputs.
\end{definition}

The marginal $\mu_\mathcal X$ defines the input distribution, and the conditional 
$\mu(\cdot \mid x)$ describes the true labeling or generative process.

The task defines a stochastic environment—a world with inputs and outputs—from 
which examples $(x,y)$ are drawn. It is intentionally abstract: it can represent 
supervised learning, reinforcement learning, or generative modeling, depending on 
the structure of $\mathcal Y$. In general, each domain $\Omega\in \mathcal D$ 
induces its own family of tasks $\mathcal T(\Omega)$, allowing a task to depend on 
the geometry of its data domain (e.g., signals on a manifold, graph, or Euclidean grid).

For example, in image classification we might have 
$\mathcal X = \R^{H\times W \times 3}$, $\mathcal Y= \{1,\dots,K\}$, and 
$\mu$ the empirical data distribution. In language modeling, we might instead have 
$\mathcal X = \Sigma^{\le n}$, $\mathcal Y = \Sigma$, and $\mu$ the joint 
next-token distribution.

\subsubsection{Capability Codomain}

To accommodate a wide class of capability types—including accuracy, loss, 
reward, perplexity, likelihood, or any task-specific utility—we do not restrict
capabilities to the unit interval $[0,1]$. Instead we work with a general ordered,
measurable, topological space that serves as the codomain for all state-level 
and expected capability maps.

\begin{definition}[Capability Codomain]\label{def:capabilitycodomain}
A \emph{capability codomain} is a quadruple
\[
(\mathcal V_\Phi,\, \preceq,\, \mathcal F_\Phi,\, \tau_\Phi)
\]
consisting of:
\begin{enumerate}
    \item a set $\mathcal V_\Phi$ equipped with a partial order $\preceq$;
    \item a $\sigma$-algebra $\mathcal F_\Phi$ making $(\mathcal V_\Phi,\mathcal F_\Phi)$ a 
    measurable space;
    \item a Hausdorff topology $\tau_\Phi$ compatible with the measurable structure;
    \item such that the following regularity condition holds:
    \begin{enumerate}
        \item (\textbf{Order--Topology Compatibility}) 
        whenever $v_n \to v$ in $\tau_\Phi$ and $v_n \preceq w$ for all $n$,
        then $v \preceq w$.
    \end{enumerate}
\end{enumerate}
\end{definition}

The codomain $\mathcal V_\Phi$ represents the values returned by a capability type 
$\Phi$; the order $\preceq$ encodes ``better vs.\ worse'' performance; the measurable
structure ensures expectations such as $m_\Phi(S,R)$ are well-defined; and the 
Hausdorff topology provides a notion of continuity compatible with the theory's 
topological assumptions on learning systems and resources.

Typical examples include:
\begin{itemize}
    \item the unit interval $([0,1],\le)$ for accuracy-like metrics;
    \item the extended nonnegative reals $([0,\infty],\le)$ for loss or perplexity;
    \item totally ordered discrete sets (e.g.\ $\{0,1\}$) for representability indicators;
    \item partially ordered performance levels with task-specific structure.
\end{itemize}
This abstraction allows the theory to treat capability values uniformly without 
restricting to any fixed numerical range.

\subsubsection{Learning Utility}

The learning task $\mathcal T$ defines the probabilistic environment, and the 
utility $\mathcal U$ defines the evaluation rule. Together they specify what it 
means to “perform well” on that capability.

\begin{definition}[Learning Utility]\label{def:learning-utility}
Given a task $\mathcal T=(\mathcal X,\mathcal Y,\mu)$ and a capability codomain 
$(\mathcal V_\Phi,\preceq,\mathcal F_\Phi,\tau_\Phi)$, a \textit{learning utility} 
is a measurable function
\[
\mathcal U : \Prob(\mathcal Y) \times \mathcal Y \longrightarrow \mathcal V_\Phi,
\]
assigning a score $\mathcal U(p,y)$ to a predicted distribution $p$
when the true label is $y$.
The utility is called \textit{regular} if it is bounded and continuous
in its first argument $p$ with respect to the weak topology on $\Prob(\mathcal Y)$.
\end{definition}

The utility $\mathcal U$ specifies what it means to "perform well." For 
log-likelihood, $\mathcal U(y',y)=\log  p(y\mid x)$, for accuracy, 
$\mathcal U(y',y)=\mathbf 1\{y'=y\}$, for reward maximization, 
$\mathcal U(y',y)$ can encode a scalar reward signal. The flexibility of 
$\mathcal U$ makes the framework agnostic to the specific nature of the task.

\subsubsection{Capability Types}

A capability type packages together the environment, value scale, and evaluation 
rule into a single object.

\begin{definition}[Capability Type]\label{def:capability-type}
    A \textit{capability type} $\Phi$ is a triple consisting of:
    \[
    \Phi := (\mathcal T, \mathcal U, \mathcal V_\Phi),
    \]
    where $\mathcal T=(\mathcal X,\mathcal Y,\mu)$ is a learning task, 
    $\mathcal V_\Phi$ is a capability codomain, and 
    $\mathcal U:\Prob(\mathcal Y)\times\mathcal Y\to\mathcal V_\Phi$ is a 
    regular learning utility.
\end{definition}

Intuitively, $\Phi$ specifies both the “world” in which the model operates and the 
metric according to which its behavior is judged. Different choices of $\Phi$ can 
describe accuracy on a benchmark, log-likelihood, BLEU score, task reward, or 
more abstract forms of performance.

\subsubsection{State-Level Capability}

Given a trained state $\theta$ (obtained via the training kernel and predictor map), 
we need a well-defined numerical quantity that says how good that state is at a 
capability. This is the \emph{state-level score} $M_\Phi(\theta)$: it evaluates one 
fixed trained model against the capability’s scoring rule.

\begin{definition}[State-Level Capability Score]\label{def:state-capability}
Let $\Phi=(\mathcal T,\mathcal U,\mathcal V_\Phi)$ denote a capability type, 
where $\mathcal T=(\mathcal X,\mathcal Y,\mu)$ is a learning task and 
$\mathcal U:\Prob(\mathcal Y)\times\mathcal Y\to\mathcal V_\Phi$ a regular utility.
For a predictor $\Pi_\theta:\mathcal X\to\Prob(\mathcal Y)$ corresponding to a 
trained state $\theta\in\Theta$, the \textit{state-level capability score} is the functional
\[
M_\Phi(\theta)
    :=\mathbb E_{(x,y)\sim\mu}\,\mathbb E_{y'\sim\Pi_\theta(\cdot|x)}\!\left[\mathcal U(y',y)\right].
\]
Equivalently, expanding this as integrals,
\[
M_\Phi(\theta)
    =\int_{\mathcal X\times \mathcal Y}\left(\int_\mathcal Y \mathcal U (y',y)\ \Pi _\theta(dy'\mid x)\right)\mu(dx,dy).
\]
\end{definition}

Because $\mathcal U$ is bounded and $\mu$ and $\Pi_\theta(\cdot\mid x)$ are probability 
measures, $M_\Phi(\theta)$ is well-defined and bounded for every $\theta\in\Theta$.

$M_\Phi(\theta)$ quantifies how well a specific model instance $\theta$ performs on 
capability type $\Phi$. In practice, it corresponds to metrics such as accuracy, 
log-likelihood, BLEU, or reward, depending on $\mathcal U$.

\subsubsection{Expected Capability}

Because training is stochastic, one state score is not enough: we care about average 
behavior across seeds and sources of randomness. The expected capability aggregates 
over randomness, producing a stable number that depends only on the system $S$ and 
resource vector $R$. This expectation is crucial; otherwise, emergence could appear 
or disappear purely due to lucky or unlucky runs.

\begin{definition}[Seed-Averaged Capability]\label{def:expected-capability}
Let $K$ be the training kernel mapping each system-resource pair $(S,R)$ to a 
distribution $K(S,R)\in \Prob(\Theta)$ over trained parameter states.
The \textit{seed-averaged capability} 
$m_\Phi:\mathcal S \times \mathcal R \to \mathcal V_\Phi$ for capability type $\Phi$ is 
\[
m_\Phi(S,R)
    := \mathbb E _{\theta\sim K(S,R)}[M_\Phi(\theta)]
    = \int_\Theta M_\Phi(\theta)\, dK(S,R)(\theta).
\]
\end{definition}

Since $M_\Phi$ is bounded and $K(S,R)$ is a probability measure, this expectation is 
always finite. The map $m_\Phi(S,R)$ represents the expected performance of a learning 
system $S\in \mathcal S$ trained with resources $R\in \mathcal R$ on the capability 
type $\Phi$. It averages over randomness in training to yield a stable, reproducible 
quantity.

This expected capability acts as the theory’s order parameter for emergence: it is 
the measurable quantity whose changes (continuous or abrupt) reveal the onset of new 
qualitative behaviors as resources, data, or architectures scale.

\subsection{Representability}

Not all learning systems can have certain abilities emerge. We cannot train an MLP with 2 layer, expecting to achieve a $m_\Phi>0.99$ on a parity capability with length 64. That's not how machine learning works, so naturally we will need tools to answer the question "when is a capability $\Phi$ representable by a learning system $S\in \mathcal S $?"

\begin{definition}
    Let $m_\Phi:\mathcal S\times \mathcal R \to \mathcal V_\Phi$ be the capability function associated with capability type $\Phi$. Define the trivial capability floor by
    \[
    c_{\min} := \inf_{R\in \mathcal R} m_\Phi(S,R)
    \]
    Then $\Phi$ is \textit{representable} by $S$ if and only if
    \[
    \sup_R m_\Phi(S,R)>c_\min
    \]
    Equivalently, $\Phi$ is \textit{representable} by $S$ if and only if there exists a $R^*\in \mathcal R$ and an open neighborhood $N(R^*)$ such that $m_\Phi(S,R)>c_\min$ for all $R\in N(R^*)$
\end{definition}

\begin{definition}{Representability Practically}{}
    Let $m_\Phi:\mathcal S\times \mathcal R \to \mathcal V_\Phi$ be the capability function associated with capability type $\Phi$, and let $\tau_\Phi :\mathcal S \times \mathcal R \to \R$ be a \textit{representability threshold functional}, assigning to each $(S,R)\in \mathcal S \times \mathcal R$ a reference value of capability requried to count as successful. Then, a capability $\Phi$ is said to be \textit{representable} by a learning system $S$ at resources $R$ if
    \[
    m_\Phi(S,R)\ge \tau_\Phi(S,R)
    \]
    The set of all such pairs is the \textit{representability region}
    \[
    \mathcal R_\Phi(S):=\{R\in \mathcal R : m_\Phi(S,R)\ge \tau_\Phi(S,R)\}
    \]
\end{definition}

Representability expresses achievability: it formalizes when a capability type can be realized by a given system under sufficient resources. The threshold $\tau_\Phi$ can be constant, task-dependent, or empirically defined (e.g., human-level accuracy, baseline reward, statistical significance). By treating it as a functional rather than a scalar constant, the definition remains fully general and coordinate-free.

The structure of the representability region $\mathcal R_\Phi(S)$ within the resource base determines where new capabilities appear. Its boundaries correspond to critical surfaces, and its one-dimensional intersections define representability thresholds along chosen resource directions.

\begin{definition}{Representability Threshold Along a Resource Axis}{}
    Let $\psi:\mathcal R \to \R^k$ be a feature map and $\alpha\in (R^k)^*$ be an efficient direction. The \textit{representability threshold} of capability type $\Phi$ for system $S$ along direction $\alpha$ is
    \[
    \rho_\Phi(S;\alpha):=\inf \{s(R)=\alpha ^\top \psi(R): m_\Phi(S,R)\ge \tau_\Phi(S,R)\}
    \]
    It gives the minimal effective resource scale along $\alpha$ at which $S$ represents capability $\Phi$.
\end{definition}

\subsubsection{Critical Surfaces}

\begin{definition}[Iso Capability]\label{def:iso-capability}
    For any fixed capability value $c\in \R$, define the \textit{iso-capability set}
    \[
    \Sigma_c(S,\Phi):= \{R\in \mathcal R:m_\Phi(S,R)=c\}
    \]
\end{definition}

Level sets of $m_\Phi(S,R)$ describe the geometry of the capability landscape. Points where $\nabla _Rm_\Phi$ changes discontinuously or the topology of $\Sigma_c$ changes define critical surfaces, which mark transitions in representability—the formal analogue of emergent “phase boundaries” in learning behavior.

\subsubsection{Gauge Equivalence}

\begin{definition}{Gauge Equivalence}{}
    Let $\psi,\psi':\mathcal \R \to \R^k$ be feature maps, $\alpha,\alpha' \in (\R^k)^*$ be directions, and let $f,f':\R\to \R$ be smooth, strictly increasing, S-shaped functions. Then, we say that the triples $(\psi,\alpha,f)$ and $(\psi' , \alpha' ,f')$ are \textit{resource-equivalent}, written
    \[
    (\psi,\alpha,f)\sim (\psi',\alpha',f')
    \]
    if and only if there exists an invertible matrix $A\in GL(k)$, a vector $c\in \R^k$, and scalars $a>0$, $b\in \R$ such that:
    \[
    \psi'(R)=A\psi(R)+c,\quad \alpha' =(A^{-1})^\top \alpha ,\quad f'(x)=f(ax+b) 
    \]
\end{definition}

This gauge equivalence tells us how different coordinate choices $\psi$, directions $\alpha$, and nonlinear profiles $f$ can be transformed into each other without changing the model’s behavior.

\subsection{Scope of the Theory}

The framework developed here aims to provide a mathematical language for describing capability emergence in large learning systems. To remain general yet precise, we delimit what phenomena are modeled and what lies beyond the present theory.

\subsubsection{In Scope}

\begin{itemize}
    \item \textbf{Learning systems} $S$ trained via stochastic optimization on large datasets, represented abstractly through data distributions and procedures.
    \item \textbf{Resource scaling $R \in \mathbb R^d_{>0}$:} we explicitly model the effect of increasing data, parameters, compute, or other resource coordinates.
    \item \textbf{Training stochasticity:} modeled at the level of distributions over trained states via the Markov kernel $K$.
    \item \textbf{Capabilities $\Phi$:} defined abstractly as tasks and evaluation utilities, with performance measured by state-level scores and seed-averaged expectations.
    \item \textbf{Emergence:} captured through the local and asymptotic behavior of capability measures$m_\Phi(S,R)$.
\end{itemize}

\subsubsection{Out of Scope For Now}

\begin{itemize}
    \item \textbf{Fine-grained optimization dynamics:} e.g. gradient noise, implicit bias of optimizers, trajectory-level analyses.
    \item \textbf{Data modality specifics:} text, vision, multimodal architectures, etc. are not distinguished; the data distribution $D$ is treated abstractly.
    \item \textbf{Human-in-the-loop effects:} reinforcement learning from human feedback, preference modeling, or interactive supervision.
    \item \textbf{Societal and alignment considerations:} while motivating, these lie outside the scope of the formal mathematics here.
\end{itemize}

\section{Axiomatizing Emergence}

\subsection{Purpose and Structure of the Axiomatic Framework}

We now specify the formal assumptions that define the space of admissible learning systems. These assumptions are divided into two tiers: structural regularity conditions, and the fundamental axioms of emergence

The following framework distinguishes between assumptions, postulates, and axioms. The assumptions ensure the mathematical well-posedness of the objects of the theory. The postulates express geometric regularities as identified by \citet{bronstein2021geometric} that define the admissible class of learning systems. The axioms (A1–A5) state the fundamental behavioral laws of emergence. While these statements are currently presented as independent, future work may investigate their logical dependencies—specifically whether the geometric postulates (G1–G3) can be derived from the emergence axioms, thereby reducing the theory to a minimal, independent core.

This makes it possible for the axioms of our theory to even speak sensibly. But conceptually, we can call only the second layer the “axioms of emergence.” The first layer is just the mathematical substrate. The axioms of the theory will depend on these base assumptions, and that's fine.

\subsection{Axioms of the Physics of Learning}

The theory rests on four axioms that jointly specify the ``physics'' governing 
how learning systems behave under scaling.  
These axioms fall naturally into three conceptual groups:

\begin{itemize}
    \item \textbf{Group I: Existence Axiom} (ensuring capabilities are not vacuous),
    \item \textbf{Group II: Structural Axioms} (continuity and monotonicity of training),
    \item \textbf{Group III: Emergent Universality} (the one-dimensional law).
\end{itemize}

Each group reflects a qualitatively different kind of assumption: logical existence, 
regularity conditions, and empirical universality.  
Together, they define the landscape on which emergence unfolds.

\subsubsection{Group I: Existence of Capability Realization}

Before one can study emergence, one must ensure that the capability in question 
is not logically or mathematically empty.  
This is encoded in a minimal existence axiom.

\begin{axiom}[Axiom of Representability]\label{axi:representability}
For every capability type $\Phi$ whose task-functional is measurable and realizable in principle, 
there exists at least one learning system $S\in\mathcal S$ and at least one resource configuration 
$R\in \mathcal R$ such that $\Phi$ is representable at $(S,R)$.
\end{axiom}

This axiom ensures that the capability function $m_\Phi$ is not the trivial constant function.  
It guarantees that the capability is realizable \emph{somewhere} in the joint space 
of learning systems and resources.  
Without this, the remaining axioms would be vacuous: one cannot study emergence if 
there is nothing that can emerge.

If removed, the entire theory could collapse to the degenerate case where every capability has 
zero (or baseline) performance regardless of resources.  
This would make representability and scaling meaningless.

\subsubsection{Group II: Structural Axioms of Learning Dynamics}

The next two axioms encode the structural regularity of 
learning dynamics: monotonicity with respect to resources and 
continuity with respect to both system and resource variations.  
These play the same role as smoothness and order axioms in classical physics.

\begin{axiom}[Axiom of Conditional Kernel Monotonicity]\label{axi:kernel-monotonicity}
Let $\Phi$ be a capability type and let $S\in\mathcal S$ be a learning system for which 
$\Phi$ is locally representable in a neighborhood of resource configurations $R_1\preceq R_2$. 
Then the training kernel $K$ is monotone-compatible with the resource order in that neighborhood:
there exists a coupling $\gamma$ of $K(S,R_1)$ and $K(S,R_2)$ such that 
\[
M_\Phi(\theta_1)\;\le\; M_\Phi(\theta_2)
\quad\text{for $\gamma$-a.e.\ } (\theta_1,\theta_2).
\]
\end{axiom}

This axiom says: \emph{increasing resources cannot make a model worse, on average}.  
The coupling formulation expresses this as a relation between distributions 
over trained states.  
It does not assume determinism — only that training with more resources yields 
a stochastically better distribution of outcomes.

If removed, capability monotonicity would no longer hold, the geometry of iso-capability surfaces 
could be pathological, and scaling laws would fail.  
This axiom is equivalent to “more training never reduces the expected skill.”

\begin{axiom}[Axiom of Kernel Continuity]\label{axi:kernel-continuity}
For any sequence $(S_n,R_n)\to (S,R)$ in the product topology of $\mathcal S\times\mathcal R$, 
the training kernel varies continuously in the weak topology:
\[
K(S_n,R_n)\;\Rightarrow\; K(S,R)
\quad\text{in } (\mathrm{Prob}(\Theta),\tau_{\mathrm{weak}}).
\]
\end{axiom}

Intuitvely, small changes to the learning system or its resource configuration 
should produce only small changes in the distribution of trained outcomes.  
This is the topological backbone that makes expected capability continuous.

If removed, the capability function $m_\Phi(S,R)$ could be discontinuous, 
tearing apart the geometry needed for representability thresholds, 
superlevel-set structure, and the existence of effective directions.

\subsubsection{Group III: Universality and the One-Dimensional Law}

The final axiom captures the empirical phenomenon that, in the regime where 
a capability is representable and sufficiently smooth,  
all multi-dimensional resource tradeoffs collapse onto a single effective axis.

\begin{axiom}[Axiom of Universality]\label{axi:universality}
There exists an equivalence class $[\psi,\alpha,f]$ such that for every learning system 
$S\in\mathcal S$ in a local neighborhood of procedures,
\[
m_\Phi(S,R)\;\approx\; f\!\left(\alpha^\top \psi(R)\right),
\]
where:
\begin{enumerate}
    \item $\psi:\mathcal R\to \mathbb R^k$ is a continuous feature map of the resource base,
    \item $\alpha\in (\mathbb R^k)^*$ is an efficient direction,
    \item $f:\mathbb R\to\mathbb R$ is a smooth, monotonically increasing scaling function.
\end{enumerate}
\end{axiom}

Although resources are multi-dimensional (data, parameters, compute, optimization steps),  
emergence depends effectively on a single scaling variable  
\[
s(R)=\alpha^\top\psi(R).
\]
Every capability collapses onto a one-dimensional manifold of effective resource.  
The invariance under affine changes of $\psi$ and $\alpha$ expresses that the law is 
about the \emph{collapse itself}, not particular coordinates.

If removed, the theory would be a purely qualitative description of capability geometry.  
There would be no reason to expect predictable scaling curves, 
no effective axes, and no universality across architectures or procedures.  
This axiom encodes the empirical ``scaling collapse'' at the heart of deep learning.

\subsection{Postulates of Geometric Regularity (G1-G3)}

In addition to the above measure-theoretic regularity, we impose three conditions. The following postulates specify the geometric structure that admissible learning systems must satisfy within the domain defined in Chapter 2. They internalize the geometric priors identified by \citet{bronstein2021geometric}) and ensure that the expected capability functional $m_\Phi(S,R)$ is invariant, stable, and hierarchical.

\begin{definition}{Geometric Regularity 1 (G1) — Symmetry}{}
There exists $G$ acting measurably on $\Omega$ and $\mathcal C$ such that for every $g\in G$,
\[
(g\cdot x )(\omega)=g_\mathcal C (x(g_\Omega^{-1}(\omega)),\quad x\in \mathcal X, \ \omega \in \Omega, \ g\in G
\]
Each predictor $\Pi_\theta :\mathcal X \to \mathcal P (\mathcal Y)$ is $G$-equivariant:
\[
\Pi_\theta(g\cdot x)=g\cdot \Pi_\theta(x),
\]
and the capability functional respects this symmetry:
\[
m_\Phi(S,R)=m_\Phi(g\cdot S, R) \quad \forall g\in G.
\]
\end{definition}

This postulate says that learning systems live in a geometric universe: every admissible system $S\in \mathcal S$ carries an invariance structure $G$ describing transformations that preserve meaning. In GDL, this corresponds to equivariance under domain symmetries. In our theory, it ensures that capability depends on intrinsic structure, not on arbitrary parameterizations.

\begin{definition}{Geometric Regularity 2 (G2) — Stability to Deformations}{}
Let $d_\mathcal D$ be a metric on the space of domains and $d_\mathcal X$ a norm on signals. Then for all $S,S'\in \mathcal S$ and fixed $R\in \mathcal R$,
\[
| m_\Phi(S,R) - m_\Phi(S',R) | \le C_\Phi\, d_\mathcal{D}(D,D') + C'_\Phi\, d_\mathcal{X}(x,x').
\]

This is a Lipschitz continuity condition of the capability functional with respect to deformations of the domain or signal. It ensures that small geometric perturbations—changes in data geometry or learned representation—produce only small changes in capability. It gives mathematical expression to robustness and continuity of learning.

This postulate sits naturally on top B7 (continuity of composition).

\end{definition}

\begin{definition}{Geometric Regularity 3 (G3) — Scale Separation}{}
There exists a filtration of the resource manifold
\[
\mathcal R_1\subset \mathcal R_2 \subset \cdots \subset \mathcal R_J =\mathcal R
\]
and corresponding projections
\[
P_j:\mathcal X \to \mathcal X_j
\]
such that capability decomposes hierarchically:
\[
m_\Phi(S,R_J)=F_\Phi(P_J,\dots,P_1),\quad m_\Phi(S,R_{j+1})\ge m_\Phi(S,R_j)
\]
and the incremental gains decrease with $j$.
\end{definition}

This expresses the hierarchical, multi-scale composition principle. It ties the resource space $\mathcal R$ to emergent capability: increasing resources extends the receptive field of the system, but returns diminish with scale.
It mirrors GDL’s “scale separation” prior and links directly to A4–A5 axioms on monotonicity and scaling laws.

The geometric regularity conditions extend the basic structural assumptions by incorporating the invariance and stability principles that \citet{bronstein2021geometric} identified as necessary for expressive architectures. The next section introduces the axioms of emergence (A1–A5), which specify how capability behaves within this structured domain.

\section{Instantiation and Empirical Realization}

The preceding chapters developed a fully abstract framework for learning systems: topological spaces of data domains and procedures, a resource base equipped with a feature map, a stochastic training kernel, predictor maps, and a seed-averaged capability functional $m_\Phi(S,R)$ that serves as an order parameter for emergence. This structure is intentionally architecture-agnostic; it is meant to describe any learning system compatible with the axioms, rather than any particular model or implementation.

A mathematical theory of an empirical domain, however, only acquires content once its objects can be realized. In probability theory, random variables become meaningful through concrete constructions on measurable spaces; in statistical mechanics, spin systems are instantiated on lattices with explicit Hamiltonians; in Geometric Deep Learning, abstract equivariant maps are realized as graph and grid neural networks. In the same spirit, the present framework must be linked to actual training pipelines before its axioms can be interpreted as more than formal constraints.

The goal of this chapter is to make this link explicit. By \emph{instantiation} we mean a systematic assignment of the abstract objects
\[
  (\Omega, P, S, R, K, \Pi_\theta, m_\Phi)
\]
to concrete choices of data distributions, architectures, optimizers, resource schedules, and evaluation metrics in real experiments. The purpose is threefold: (i) to demonstrate that the axioms can be satisfied by actual systems used in practice, (ii) to provide a reusable template for mapping arbitrary ML workflows into the formalism, and (iii) to prepare the ground for falsification-focused experiments.

Throughout this chapter, “instantiation” is not treated as an afterthought or a collection of illustrative examples. Instead, it is a central component of the theory: it specifies how the abstract learning space $S$, the resource base $R$, and the capability functional $m_\Phi$ interface with code. Once this interface is established, the theorems of Chapter~\ref{chap:theoretical-consequences} become empirical predictions about how particular training pipelines should behave under scaling, and failures of those predictions become information about the limits of the axioms.

\subsection{Purpose and Scope of Instantiation}

The notion of a learning system $S = (\Omega,P)$ as a point in a topological space, a resource vector $R \in \mathcal{R}$ as an element of a topological ordered monoid, and a training process as a Markov kernel
\[
  K : (S \times \mathcal{R}, \tau_S \times \tau_{\mathcal{R}})
  \rightsquigarrow (\Prob(\Theta), \tau_{\mathrm{weak}})
\]
is deliberately abstract. This level of generality is necessary to state the axioms of emergence—monotonicity, universality, representability, and continuity—in a coordinate-free manner, and to prove the collapse theorem linking $m_\Phi(S,R)$ to a one-dimensional effective resource axis.

At the same time, this abstraction raises a natural question: \emph{what are these objects in practice?} What, concretely, does it mean to take a tuple
\[
  S = (\Omega, P) =
  (\text{data distribution}, \text{training procedure})
\]
and treat it as a single point in a topological space? How do standard components of modern machine learning—such as a Transformer architecture with AdamW optimization on a modular addition task—appear inside this formalism?

The purpose of instantiation is to answer these questions in a disciplined way. Concretely, an instantiation has the following roles:
\begin{enumerate}
  \item \textbf{Semantic grounding.} It ties each abstract definition to a family
  of real-world objects (datasets, models, training loops), ensuring that the
  formalism describes the intended empirical domain and is not vacuous.

  \item \textbf{Operational usability.} It provides a repeatable recipe for
  mapping arbitrary training setups into the theory: given a piece of code, one
  can identify the associated $S$, $R$, $K$, $\Pi_\theta$, and $m_\Phi$.

  \item \textbf{Prediction interface.} It specifies where theorems “touch” reality:
  once an experiment is instantiated, statements such as monotone capability,
  local continuity, or one-dimensional collapse become concrete predictions about
  measurable quantities.

  \item \textbf{Falsification and refinement.} By exhibiting explicit systems
  and computing their capabilities under scaling, one can locate where the axioms
  fail, thereby refining the scope of the theory and suggesting modifications or
  extensions.
\end{enumerate}

The scope of the present chapter is restricted to instantiations that are sufficiently simple to be analyzed clearly, yet rich enough to exercise every part of the formalism. In particular, we focus on small-scale controlled tasks such as modular addition and parity, along with minimal deep-learning pipelines that already resemble those used in practice. These examples will be revisited in the chapter of experiments as the basis for empirical tests of the axioms.

\subsection{A Framework for Instantiating Learning Systems}
\label{subsec:instantiation-framework}

To keep the connection between theory and implementation systematic, each instantiation in this chapter follows a common pattern. Given a concrete training setup written in code, we extract the following seven objects:

\begin{enumerate}
  \item \textbf{Data domain $(\Omega,\tau_\Omega)$.}
  The set of possible inputs (and, when relevant, input--output pairs) together
  with its topology. In practice, $\Omega$ may be a finite set (e.g.\ discrete
  tokens or images), a subset of $\R^n$, or a product space. The data-generating
  distribution $D$ on $\Omega$ equips it with a natural measurable structure.

  \item \textbf{Procedure $(P,\tau_P)$.}
  A specification of how models are built and trained: architecture family,
  loss function, optimizer, initialization scheme, regularization, and any
  hyperparameters that control them. We view $P$ as a point in a product space
  of categorical and continuous components endowed with the procedure topology
  of Definition~\ref{def:procedure-distance}, so that small perturbations in
  hyperparameters correspond to nearby procedures.

  \item \textbf{Learning system $S = (\Omega,P)$.}
  The pair of data domain and procedure, regarded as a point in the product
  space $S = \mathcal{D} \times \mathcal{P}$ with the product topology
  (Axiom~\ref{ax:learning-system-topology}). This representation allows us to
  talk about neighborhoods of “similar” systems and continuous deformations
  between them.

  \item \textbf{Resource base $(\mathcal{R},\oplus,0,\preceq,\tau_{\mathcal{R}})$.}
  The coordinates that control scale: number of parameters, tokens, optimization
  steps, or other resource-like quantities. For each instantiation we specify:
  \begin{itemize}
    \item the concrete resource vector $R \in \R^d_{>0}$,
    \item the combination operation $\oplus$ (often componentwise addition),
    \item the partial order $\preceq$ capturing “no less resource,” and
    \item a feature map $\psi : \mathcal{R} \to \R^k$ that embeds resources into
          Euclidean coordinates (Definition~\ref{def:feature-map}).
  \end{itemize}
  In simple cases we take $\mathcal{R} = \R^d_{>0}$ with $\psi(R) = \log R$,
  turning multiplicative scaling into additive coordinates.

  \item \textbf{Training kernel $K(S,R)$.}
  The stochastic training process, represented as a Markov kernel from
  $S \times \mathcal{R}$ to $\Prob(\Theta)$ (Axiom~\ref{ax:training-kernel}). In
  code, this corresponds to randomized initialization, data shuffling, and any
  other randomness in optimization. Given $(S,R)$, a single training run samples
  a parameter state $\theta \sim K(S,R)$; repeating the run with different seeds
  draws from the same kernel.

  \item \textbf{Predictor map $\Pi_\theta$.}
  For each trained state $\theta \in \Theta$, the associated predictor
  $\Pi_\theta : X \to \Prob(Y)$ maps inputs to predictive distributions over
  outputs (Definition~\ref{def:predictor-map}). In practice, $\Pi_\theta$ is
  implemented by a neural network followed by a softmax, logistic sigmoid, or
  other output layer.

  \item \textbf{Capability functional $m_\Phi(S,R)$.}
  A capability type $\Phi$ specifies a task and a bounded utility function
  $U_\Phi$ that scores predictions against ground truth. Given $\theta$ and
  $\Phi$, the state-level score $M_\Phi(\theta)$ is the expected utility over
  data, and the seed-averaged capability
  \[
    m_\Phi(S,R)
      = \E_{\theta \sim K(S,R)}[ M_\Phi(\theta) ]
  \]
  aggregates over training randomness (Definition~\ref{def:expected-capability}).
  This is the quantity to which the axioms and the collapse theorem apply.
\end{enumerate}

For each concrete example (e.g.\ modular addition, parity, or small language models), the subsections that follow will instantiate these seven objects explicitly. The intention is that, given any future training pipeline, one can repeat the same extraction procedure: identify $(\Omega,P)$, specify the resource coordinates $R$ and feature map $\psi$, define the corresponding kernel $K$, and thus obtain a well-defined capability map $m_\Phi(S,R)$.

This structured approach ensures that the theory remains usable. Rather than treating the abstract definitions as detached from practice, we treat each experiment as a point in the same learning space on which the axioms are stated. Once this identification is made, statements like “capability is monotone in resources,” “capability is continuous in procedure-local neighborhoods,” or “capability collapses to a one-dimensional effective resource axis” become concrete, testable claims about the instantiated system.

\subsection{Structural Instantiation}

\subsubsection{Instantiating the Data Domain}

\subsubsection{Instantiating the Procedure Space}

\subsubsection{Instantiating the Resource Base}

\begin{example}[Canonical logarithmic resource coordinates]\label{ex:log-coordinates}
Consider the concrete resource base $\mathcal R=\R^d_{>0}$ equipped with 
componentwise multiplication as the resource composition,
\[
(R_1\oplus R_2)^i \;=\; R_1^i R_2^i,
\]
and the usual product order
$R_1\preceq R_2$ iff $R_1^i \le R_2^i$ for all coordinates $i$.
Define a coordinate map
\[
\psi:\R^d_{>0}\to\R^d,
\qquad 
\psi(R)=\log R,
\]
where the logarithm is taken componentwise.
Then:
\begin{enumerate}
    \item \textbf{Monoid homomorphism:}
    \[
    \psi(R_1\oplus R_2)
      =\log(R_1\odot R_2)
      =\log R_1 + \log R_2
      =\psi(R_1)+\psi(R_2),
    \]
    so multiplicative composition becomes additive in~$\R^d$.

    \item \textbf{Order preservation:}
    \[
    R_1\preceq R_2 \quad\Longleftrightarrow\quad 
    R_1^i\le R_2^i \text{ for all } i
    \quad\Longleftrightarrow\quad 
    \log R_1^i \le \log R_2^i \text{ for all } i
    \quad\Longleftrightarrow\quad 
    \psi(R_1)\le \psi(R_2),
    \]
    so $\psi$ preserves (and reflects) the resource order.

    \item \textbf{Continuity:}
    Since the logarithm is continuous on $\R_{>0}$, the map $\psi$ is continuous with
    respect to the standard topology.
\end{enumerate}
Thus the choice $\psi=\log$ provides a concrete realization of the
representability condition in Axiom~\ref{axi:representability}: multiplicative scaling of
resources in $\R^d_{>0}$ is represented as additive translation in the
coordinate space~$\R^d$.
\end{example}


\subsubsection{Instantiating the Training Kernel}

\subsubsection{Instantiating the Predictor Map}

\subsubsection{Instantiating a Capability}

\subsection{Instantiation in Modern Training Pipelines}

\subsubsection{A Typical Classification Pipeline}

\subsubsection{Identifying the Learning System}

\subsubsection{Identifying the Resource Coordinates}

\subsubsection{Identifying the Training Kernel in Code}

\subsubsection{Identifying the Capability Functional}

\subsection{Predictive Consequences}

\subsubsection{Monotonicity Prediction}

\subsubsection{Continuity Prediction}

\subsubsection{1D Collapse Prediction}

\subsubsection{Representability Threshold Prediction}

\section{Theoretical Consequences}

\subsection{The Scaling Law}

\subsubsection{Continuity of Capability}

\begin{lemma}[Boundedness and continuity of the state-level capability]\label{lem:Mphi-cont}
The state-level capability
\[
M_\Phi(\theta)
  = \E_{(x,y)\sim\mu}
    \E_{y'\sim\Pi_\theta(\cdot|x)}[\,\mathcal U(y',y)\,]
\]
is bounded and continuous on $\Theta$.
\end{lemma}
\begin{proof}
Boundedness follows immediately from the Definition of Utility \ref{def:learning-utility}: Since 
\[
|\mathcal U(y',y)|\le \|\mathcal U\|_\infty
\]
for all \((y',y)\), we have
\[
|M_\Phi(\theta)|
  = \left|\E_{(x,y)\sim\mu}\,\E_{y'\sim\Pi_\theta(\cdot\mid x)}
         [\mathcal U(y',y)]\right|
  \le \|\mathcal U\|_\infty.
\]

For continuity, let $\theta_n\to\theta$. By Axiom \ref{axi:kernel-continuity},
\[
\Pi_{\theta_n}(\cdot\mid x)\Rightarrow \Pi_\theta(\cdot\mid x)
\quad\text{for each }x.
\]
Since $\mathcal U$ is bounded and continuous in its first argument,
\[
\E_{y'\sim\Pi_{\theta_n}(\cdot\mid x)}[\mathcal U(y',y)]
   \longrightarrow
\E_{y'\sim\Pi_\theta(\cdot\mid x)}[\mathcal U(y',y)]
\quad\text{for each }(x,y).
\]

To pass the limit through the outer $\mu$-integral, recall from Definition of State Capability \ref{def:state-capability} that
\[
M_\Phi(\theta)
  = \int_{\mathcal X\times\mathcal Y}
      \left(
         \int_{\mathcal Y} \mathcal U(y',y)\,
         \Pi_\theta(dy'\mid x)
      \right)\mu(dx,dy).
\]

For each $n$, define
\[
f_n(x,y)
  :=
  \int_{\mathcal Y} \mathcal U(y',y)\,
     \Pi_{\theta_n}(dy'\mid x),
\quad
f(x,y)
  :=
  \int_{\mathcal Y} \mathcal U(y',y)\,
     \Pi_{\theta}(dy'\mid x).
\]
Then
\[
M_\Phi(\theta_n)=\int_{\mathcal X\times\mathcal Y} f_n(x,y)\,d\mu(x,y),
\quad
M_\Phi(\theta)=\int_{\mathcal X\times\mathcal Y} f(x,y)\,d\mu(x,y).
\]

We have $f_n(x,y)\to f(x,y)$ for every $(x,y)$, and by boundedness of $\mathcal U$,
\[
|f_n(x,y)|\le \|\mathcal U\|_\infty
\quad\text{for all }n,(x,y).
\]
Since $f_n(x,y)\to f(x,y)$ pointwise, the dominated
convergence theorem (Appendix \ref{app:dct} yields
\[
\int_{\mathcal X\times\mathcal Y} f_n(x,y)\,d\mu(x,y)
   \;\longrightarrow\;
\int_{\mathcal X\times\mathcal Y} f(x,y)\,d\mu(x,y).
\]
Thus $M_\Phi(\theta_n)\to M_\Phi(\theta)$.
\end{proof}


\begin{lemma}[Local continuity of the expected capability]\label{lem:mphi-cont}
The expected capability
\[
m_\Phi(S,R)
  = \int_\Theta M_\Phi(\theta)\,dK(S,R)(\theta)
\]
is continuous on procedure-local neighborhoods.
\end{lemma}

\begin{proof}
\emph{continuity of $m_\Phi$ via Portmanteau.}
Fix a procedure-local neighborhood $U\subseteq\mathcal S\times\mathcal R$.
Let $(S_n,R_n)\to(S,R)$ with $(S_n,R_n),(S,R)\in U$.
By Axiom \ref{axi:kernel-continuity},
\[
K(S_n,R_n)\ \Rightarrow\ K(S,R)\qquad\text{in }(\Prob(\Theta),\tau_{\rm weak}).
\]
Since $M_\Phi$ is bounded and continuous by Lemma \ref{lem:Mphi-cont}, the Portmanteau theorem yields
\[
\int_\Theta M_\Phi(\theta)\,dK(S_n,R_n)(\theta)\ \longrightarrow\
\int_\Theta M_\Phi(\theta)\,dK(S,R)(\theta).
\]
That is, $m_\Phi(S_n,R_n)\to m_\Phi(S,R)$. Thus $m_\Phi$ is sequentially continuous on $U$. Because the procedure-local topology on $\mathcal S\times\mathcal R$ is first-countable (hence sequential), sequential continuity implies continuity. Therefore $m_\Phi$ is continuous on $U$.

\end{proof}

In summary:

\begin{theorem}[Continuity of Capability]\label{thm:capability-cont}
Under Basic Regularities \textnormal{(B1)--(B9)},
the capability functions $M_\Phi$ and $m_\Phi$
are bounded and continuous on their respective domains.
\end{theorem}

\subsubsection{Capability Monotonicity}

\begin{theorem}[Capability Monotonicity]\label{thm:capability-monotonicity}
Fix a capability type $\Phi$ and a system $S\in\mathcal S$.
If $R_1\preceq R_2$ in the resource base $(\mathcal R,\oplus,0,\preceq,\tau_{\mathcal R})$, then
\[
m_\Phi(S,R_1)\;\le\; m_\Phi(S,R_2)
\]
\end{theorem}

\begin{proof}
By the Axiom of Kernel Monotonicity \ref{axi:kernel-monotonicity}, the training kernel $K$ is monotone compatible with the resource order:
$R_1\preceq R_2$ implies the existence of a coupling $\gamma$ of $K(S,R_1)$ and $K(S,R_2)$ such that
$M_\Phi(\theta_1)\le M_\Phi(\theta_2)$ for $\gamma$-a.e.\ $(\theta_1,\theta_2)$.
Taking expectations under $\gamma$ gives
\[
\mathbb E_{\theta_1\sim K(S,R_1)}[M_\Phi(\theta_1)]
\;\le\;
\mathbb E_{\theta_2\sim K(S,R_2)}[M_\Phi(\theta_2)],
\]
hence $m_\Phi(S,R_1)\le m_\Phi(S,R_2)$.
\end{proof}

\begin{corollary}[Upward-closed superlevel sets; frontier = level set; codimension-1]\label{cor:upward-frontier}
Fix $\Phi$ and $S$. For any $c\in\mathbb R$, the superlevel set
\[
U_c(S,\Phi):=\{R\in\mathcal R:\; m_\Phi(S,R)\ge c\}
\]
is upward-closed: if $R\in U_c$ and $R\preceq R'$, then $R'\in U_c$.
If, in addition, $m_\Phi(S,\cdot)$ is continuous on a neighborhood of $U_c$, then
\[
\Sigma_c(S,\Phi):=\{R:\; m_\Phi(S,R)=c\}=\partial U_c.
\]
If furthermore $m_\Phi(S,\cdot)$ is $C^1$ near $\Sigma_c$ and $c$ is a regular value
($\nabla_R m_\Phi(S,R)\neq 0$ on $\Sigma_c$ in any Euclidean chart of a feature map),
then $\Sigma_c(S,\Phi)$ is a codimension-$1$ submanifold (hypersurface) of the resource chart.
\end{corollary}

\begin{proof}
Upward-closedness: let $R\in U_c$ and $R\preceq R'$. By Theorem~\ref{thm:capability-monotonicity},
\[
m_\Phi(S,R')\ge m_\Phi(S,R)\ge c,
\]
hence $R'\in U_c$.

Frontier = level set: continuity of $m_\Phi(S,\cdot)$ implies
$U_c=\{m_\Phi\ge c\}$ is closed (preimage of $[c,\infty)$); therefore its boundary equals
$\{m_\Phi=c\}=\Sigma_c$.

Codimension-1: in any Euclidean chart provided by the feature map,
if $m_\Phi(S,\cdot)$ is $C^1$ and $c$ is a regular value, the regular level set theorem yields
that $\Sigma_c$ is a $(k-1)$-dimensional embedded submanifold, i.e.\ codimension $1$.
\end{proof}

\subsubsection{Existence of an Effective Direction}

\begin{lemma}[Universality implies existence of an effective direction]\label{lem:effective-direction}
Suppose Axiom~\ref{axi:universality} (Universality) holds on a procedure-local neighborhood
of $(S,\Phi)$. That is, there exists a triple $[\psi,\alpha,f]$ such that
\[
m_\Phi(S,R)
\;=\;
f\!\left(\frac{\alpha^\top \psi(R)-s^*(S)}{w(S)}\right)
\]
for all $R$ in that neighborhood, and this representation is stable under
resource-equivalence. Then there exists an equivalence class of nonzero
directions $[\alpha]\subset (\R^k)^*$ such that, after quotienting by
resource-equivalence, the iso-capability sets are locally hyperplanes
\[
\{\,R\in\mathcal R:\ \alpha^\top \psi(R)=\text{\rm const.}\,\}
\]
in the resource coordinates $\psi(R)$. Equivalently, in this neighborhood the
capability depends only on the single scalar
\[
s(R) \;=\; \alpha^\top \psi(R),
\]
up to admissible affine reparameterizations of $s$.
\end{lemma}

\begin{proof}
Fix a procedure-local neighborhood $U\subseteq\mathcal S\times\mathcal R$ of
$(S,\Phi)$ on which the Axiom of Universality~\ref{axi:universality} holds with some triple
$[\psi,\alpha,f]$. For the moment, fix the state $S$ and consider the induced
neighborhood
\[
U_S \;:=\; \{\,R\in\mathcal R : (S,R)\in U\,\}.
\]

\medskip
\noindent\textbf{Step 1: Resource-equivalence as a level-set relation.}
On $U_S$, recall the definition of resource equivalence (Definition \ref{def:cap-equiv}):
\[
R \sim R'
\quad\Longleftrightarrow\quad
m_\Phi(S,R) = m_\Phi(S,R').
\]
By definition, iso-capability (Definition \ref{def:iso-capability} sets are precisely the equivalence classes of
$\sim$.

By Universality \ref{axi:universality}, there exist scalars $s^*(S)\in\R$ and $w(S)>0$ and a universal profile $f:\R\to\R$ such that
\[
m_\Phi(S,R)
\;=\;
f\!\left(\frac{\alpha^\top \psi(R)-s^*(S)}{w(S)}\right)
\quad\text{for all }R\in U_S.
\]
Assume $f$ is strictly monotone on the relevant range (as part of the
universality axiom). Then for any $R,R'\in U_S$,
\[
\begin{aligned}
m_\Phi(S,R) = m_\Phi(S,R')
&\quad\Longleftrightarrow\quad
\frac{\alpha^\top \psi(R)-s^*(S)}{w(S)}
=
\frac{\alpha^\top \psi(R')-s^*(S)}{w(S)}\\
&\quad\Longleftrightarrow\quad
\alpha^\top \psi(R)=\alpha^\top \psi(R').
\end{aligned}
\]

Thus the equivalence classes of $\sim$ are exactly the level sets of the scalar
\[
s(R) \;:=\; \alpha^\top \psi(R).
\]

\medskip
\noindent\textbf{Step 2: Iso-capability sets as hyperplanes in resource coordinates.}
Let $z=\psi(R)\in\R^k$ denote the resource coordinates. For any constant
$c\in\R$, the corresponding iso-capability set at level

 \[
 c_0 :=
f\left(\frac{c-s^*(S)}{w(S)}\right)
 \]

is
\[
\begin{aligned}
\{\,R\in U_S : m_\Phi(S,R)=c_0\,\}&
=
\{\,R\in U_S : s(R)=c\,\}\\
&=
\{\,R\in U_S : \alpha^\top \psi(R)=c\,\}.
\end{aligned}
\]
In the coordinate space $\psi(U_S)\subseteq\R^k$, this is the intersection of
$\psi(U_S)$ with the affine hyperplane
\[
H_c \;:=\; \{\,z\in\R^k : \alpha^\top z = c\,\}.
\]
Since $\alpha\neq 0$ (otherwise $m_\Phi$ would be constant in $R$ on $U_S$ and
the universality representation would be degenerate), each $H_c$ is a
codimension-one hyperplane in $\R^k$.

Thus, after quotienting $U_S$ by the equivalence relation $\sim$, the
iso-capability classes are locally represented by hyperplanes
$\{R : \alpha^\top \psi(R)=\text{const.}\}$ in the resource coordinates.

\medskip
\noindent\textbf{Step 3: Effective direction up to affine reparameterization.}
The scalar
\[
s(R) = \alpha^\top \psi(R)
\]
is defined only up to affine reparameterizations. Indeed, replacing $s$ by
\[
\tilde{s}(R)
  := \frac{s(R)-s^*(S)}{w(S)}
\]
and correspondingly composing $f$ with an affine change of variables does not
change $m_\Phi(S,R)$. Likewise, replacing $\alpha$ by any nonzero scalar
multiple $\lambda\alpha$ and defining
\[
\tilde{f}(u) := f(u/\lambda)
\]
leads to an equivalent representation of the same capability function. Thus the
effective direction is determined only up to nonzero scalar multiples; we
denote this equivalence class by $[\alpha]\subset (\R^k)^*$.

In summary, universality on $U$ implies the existence of a (projective) class
of nonzero directions $[\alpha]$ such that:
\begin{enumerate}
    \item iso-capability sets coincide with level sets of $s(R)=\alpha^\top\psi(R)$;
    \item these level sets are (local) hyperplanes in the coordinate space; and
    \item $m_\Phi$ depends on $R$ only through $s(R)$, up to affine
          reparameterization of $s$.
\end{enumerate}
This is exactly the claimed existence of an effective direction.
\end{proof}


\begin{lemma}[Well-defined scaling curve]\label{lem:scaling-curve}
Let $S$ be fixed and let $\alpha$ be an effective direction as in Lemma \ref{lem:effective-direction}.
Define the scalar
\[
s(R):=\alpha^\top\psi(R).
\]
Then the map
\[
f_S:\ s\mapsto m_\Phi(S,R)\qquad\text{for any }R\text{ with }s(R)=s
\]
is well-defined, monotone, and continuous.
Consequently,
\[
m_\Phi(S,R)=f_S\bigl(\alpha^\top\psi(R)\bigr)
\]
up to admissible affine reparameterizations of $s$.
\end{lemma}

\begin{proof}
\emph{Well-definedness.}
By Lemma \ref{lem:effective-direction},
\[
R\sim R'
\quad\Longleftrightarrow\quad
s(R)=s(R').
\]
Thus, if $s(R)=s(R')$, then $m_\Phi(S,R)=m_\Phi(S,R')$, so $f_S$ is well-defined.

\medskip
\noindent\emph{Monotonicity.}
By Universality \ref{axi:universality}, there exists a strictly monotone profile $f$ such that
\[
m_\Phi(S,R)=f\!\left(\frac{s(R)-s^*(S)}{w(S)}\right)
\]
on a procedure-local neighborhood.
Since $s(R)$ is an affine functional of $\psi(R)$ and $f$ is strictly monotone, the resulting $f_S$ is monotone in~$s$.

\medskip
\noindent\emph{Continuity.}
By Theorem \ref{thm:capability-cont}, $R\mapsto m_\Phi(S,R)$ is continuous. Since
\[
R\mapsto s(R)=\alpha^\top\psi(R)
\]
is continuous, the quotient map $R\mapsto s(R)$ is continuous, and therefore the induced single-variable function $f_S$ is continuous.

This establishes the representation
\[
m_\Phi(S,R)=f_S(s(R))
=f_S(\alpha^\top\psi(R)),
\]
up to the admissible affine reparameterizations allowed in Universality \ref{axi:universality}. 
\end{proof}

\subsubsection{The Scaling Law}

\begin{theorem}[Scaling law]\label{thm:scaling-law}
Let $(S,\Phi)$ be an admissible learning system. Then there exists a procedure-local
scaling map $f_S$ such that
\[
m_\Phi(S,R)
  = f_S\bigl(\alpha^\top \psi(R)\bigr)
\]
for all $R$ in a procedure-local neighborhood of $S$, up to admissible affine reparameterizations of the scalar
\[
s(R)=\alpha^\top\psi(R).
\]
\end{theorem}

\begin{proof}
Fix an admissible learning system $(S,\Phi)$. By Universality \ref{axi:universality}, there exists a procedure-local
neighborhood $U\subseteq\mathcal S\times\mathcal R$ of $(S,\Phi)$ and a representation triple $[\psi,\alpha,f]$ such that
\[
m_\Phi(S,R)
  = f\!\left(\frac{\alpha^\top\psi(R)-s^*(S)}{w(S)}\right)
\]
for all $R$ with $(S,R)\in U$, and this representation is stable under gauge equivalence.

By Lemma \ref{lem:effective-direction}, there exists an effective direction $\alpha\neq 0$ such that, after quotienting by capability equivalence at $S$,
the iso-capability sets
\[
\{\,R\in\mathcal R : m_\Phi(S,R)=\text{\rm const.}\,\}
\]
are locally given by hyperplanes
\[
\{\,R\in\mathcal R : \alpha^\top\psi(R)=\text{\rm const.}\,\}
\]
in the resource coordinates $\psi(R)$.
Equivalently, in this neighborhood the capability depends on $R$ only through
the scalar
\[
s(R) := \alpha^\top\psi(R),
\]
up to admissible affine reparameterizations of $s$.

Now fix $S$ and consider the induced neighborhood
\[
U_S := \{\,R\in\mathcal R : (S,R)\in U\,\}.
\]
Define $f_S$ on the image
\[
I_S := s(U_S)\subseteq\R
\]
by
\[
f_S(s) \coloneqq m_\Phi(S,R)
\quad\text{for any }R\in U_S\text{ with }s(R)=s.
\]
By Lemma~\ref{lem:scaling-curve}, this definition is well-defined (i.e.
independent of the choice of $R$ with $s(R)=s$), and the resulting map $f_S$
is monotone and continuous in $s$. Moreover, Lemma~\ref{lem:scaling-curve}
shows that
\[
m_\Phi(S,R) = f_S\bigl(s(R)\bigr)
             = f_S\bigl(\alpha^\top\psi(R)\bigr)
\]
for all $R\in U_S$, up to the admissible affine reparameterizations of $s$
allowed by Universality~\ref{axi:universality} (i.e.\ rescalings and shifts of $s$ absorbed
into a corresponding redefinition of $f_S$).

This is exactly the claimed scaling law for the fixed system $S$.
Since $S\in\mathcal S$ was arbitrary, the conclusion holds for each admissible
learning system.
\end{proof}

\subsection{Criticality and Phase Geometry}

\subsection{Backpropagation as a Derived Principle}
https://chatgpt.com/g/g-p-68a088c3c5b081918671bb266c308f51-ai-research/c/690ab849-f1b0-8331-9904-ba89cf358a53

\subsection{Universality and Resource Equivalence}

\section{Experimentation—Trying My Absolute Best To Break My Theory}

When you axiomatize an empirical domain like modern AI, you’re not declaring truths, but instead you’re proposing tight, testable axioms that compress messy behavior into a few operational claims. Each axiom turns vague talk of “emergence” into specific predictions about a continuous order parameter that should rise along an effective resource axis, align across different ways of spending resources, stay stable within a defined training regime, and be immune to arbitrary reparameterization. Making these axioms falsifiable maximizes information gain. A confirming result means the lens has real predictive bite; a failure tells you exactly where the world refuses the simplification—which knob (data, parameters, steps, context length, precision, data quality) breaks curve collapse, which metric creates mirages, which procedure shift moves the onset or sharpness. In this hybrid program, experiments don’t just “validate” theory; they carve its domain of validity, turning broad conjectures into precise, procedure-local laws and revealing the minimal assumptions under which capability transitions actually behave like scaling phenomena worth proving later.

\clearpage

\appendix
\section{Mathematical Appendix}
\label{app:mathdefs}

\subsection{Probability- and Measure Theory}

\subsubsection{Measurable Structure}


\begin{definition}[$\sigma$-Algebra]\label{app:sigma-algebra}
Let $X$ be a nonempty set.  
A \textit{$\sigma$-algebra} $\mathcal F$ on $X$ is a collection of subsets of $X$ satisfying:
\begin{enumerate}
    \item $X \in \mathcal F$,
    \item if $A \in \mathcal F$, then $A^c = X \setminus A \in \mathcal F$,
    \item if $\{A_i\}_{i=1}^\infty \subseteq \mathcal F$, then $\bigcup_{i=1}^\infty A_i \in \mathcal F$.
\end{enumerate}
Elements of $\mathcal F$ are called \textit{measurable sets}.  
The $\sigma$-algebra encodes the subsets of $X$ to which a measure may consistently assign sizes or probabilities.  
The smallest $\sigma$-algebra containing a family $\mathcal G \subseteq 2^X$ is called the \textit{$\sigma$-algebra generated by} $\mathcal G$, written $\sigma(\mathcal G)$.
\end{definition}

\begin{definition}[Measurable Space]\label{app:measurable-space}
A \textit{measurable space} is a pair $(X,\mathcal F)$, where $X$ is a set and $\mathcal F$ is a $\sigma$-algebra on $X$.  
It provides the structural notion of which subsets of $X$ are admissible for measure and integration.
\end{definition}

\begin{definition}[Measure]\label{app:measure}
Let $(X,\mathcal F)$ be a measurable space.  
A \textit{measure} is a function $\mu : \mathcal F \to [0,\infty]$ such that:
\begin{enumerate}
    \item $\mu(\emptyset) = 0$,
    \item (Countable additivity) if $\{A_i\}_{i=1}^\infty$ are disjoint in $\mathcal F$, then
    \[
    \mu\!\left(\bigcup_{i=1}^\infty A_i\right) = \sum_{i=1}^\infty \mu(A_i).
    \]
\end{enumerate}
A measure assigns "size" or "mass" to measurable sets in a consistent way.  
If $\mu(X) = 1$, we call $\mu$ a \textit{probability measure} and $(X,\mathcal F,\mu)$ a \textit{probability space}.
\end{definition}

\begin{definition}[Measurable Map]\label{app:measurable-map}
Let $(X,\mathcal F_X)$ and $(Y,\mathcal F_Y)$ be measurable spaces.  
A function $f : X \to Y$ is \textit{measurable} if
\[
f^{-1}(A) \in \mathcal F_X \quad \text{for all } A \in \mathcal F_Y.
\]
That is, the preimage of every measurable set in $Y$ is measurable in $X$.  
This ensures that measures defined on $X$ and $Y$ interact coherently under $f$, enabling the construction of \textit{pushforward measures}
\[
f_\# \mu(A) = \mu(f^{-1}(A))
\]
and the definition of integrals $\int f\, d\mu$.
\end{definition}

The concept of measurability provides the bridge between topology and probability.  
It ensures that mappings between domains—such as signals $x : \Omega \to C$ in the main text—preserve the structure necessary to define probabilities and expectations.  
Without measurability, integrals like $\mathbb E[x]$ or probability measures on images $x(\Omega)$ would be undefined.

In the main theory, every space that carries probabilistic structure (data domains, parameter spaces, value spaces) is equipped with a Borel $\sigma$-algebra generated by its topology. Measurable maps—signals, kernels, and predictors—are functions that respect these structures.

\subsubsection{Borel Structure}
\label{app:borel}

\begin{definition}[Borel $\sigma$-algebra]
Let $(X,\tau)$ be a topological space.  
The \emph{Borel $\sigma$-algebra} on $X$, denoted $\mathscr B(X)$, is the smallest $\sigma$-algebra containing all open sets of $X$.  
The elements of $\mathscr B(X)$ are called \emph{Borel sets}.
\end{definition}

The Borel $\sigma$-algebra is the minimal collection of subsets of $X$ to which one can assign probabilities or measures while remaining consistent with the topology.  
It allows us to integrate and talk about probabilities without leaving the structure of the underlying space.  
In our framework, $\Theta$, $\mathcal S$, and $\mathcal R$ all carry their natural topologies, and we use their corresponding Borel $\sigma$-algebras to define measurable maps such as the training kernel
\[
K:(\mathcal S\times\mathcal R,\B(\mathcal S\times\mathcal R))
  \longrightarrow (\Prob(\Theta),\B(\Prob(\Theta))) ,
\]
where:
\[
\B(\Prob(\Theta)):=\text{Borel}(\tau_{\mathrm{weak}})
\]

If $\mathcal S$ and $\mathcal R$ are Polish, then
$\B(\mathcal S\times\mathcal R)=\B(\mathcal S)\times\B(\mathcal R)$.

\begin{definition}[Borel measurable function]
Let $(X,\mathscr A)$ and $(Y,\mathscr B)$ be measurable spaces.  
A map $f:X\to Y$ is \emph{(Borel) measurable} if for every $B\in\mathscr B$,
\[
f^{-1}(B)\in \mathscr A.
\]
\end{definition}

Measurability ensures that the pre-image of any event we can \textit{measure} in $Y$ remains measurable in $X$.  
It is the minimal condition required for integrals like $\int f\,d\mu$ to make sense.  

A measurable function $f:X\to\mathbb R$ is \textit{bounded} if $\sup_{x\in X}|f(x)|<\infty$.  
Boundedness guarantees that integrals against probability measures are finite, a property used throughout the theory for quantities such as $M_\Phi(\theta)$ and $m_\Phi(S,R)$.

\textbf{Relevance in this theory.}
\begin{itemize}
    \item The capability function $M_\Phi:\Theta\to\mathbb R$ must be Borel measurable so that expectations $\int M_\Phi\,dK(S,R)$ are well-defined.
    \item The kernel $K$ itself is measurable in the sense that $(S,R)\mapsto K(S,R)(A)$ is measurable for every Borel set $A\subseteq\Theta$.
    \item Boundedness of $M_\Phi$ ensures that all integrals and expectations are finite and continuous under weak convergence.
\end{itemize}

\subsubsection{Probability Spaces and Measures}
\label{app:probability-spaces-measures}

\begin{definition}{Measure space}{}
A \emph{measure space} is a triple $(X,\mathscr A,\mu)$ where:
\begin{itemize}
    \item $X$ is a set (the underlying space),
    \item $\mathscr A$ is a $\sigma$-algebra of subsets of $X$ (the measurable sets),
    \item $\mu:\mathscr A\to[0,\infty]$ is a measure satisfying:
    \begin{enumerate}
        \item $\mu(\emptyset)=0$,
        \item For any countable family of disjoint sets $(A_i)_{i=1}^\infty\subseteq\mathscr A$,
        \[
        \mu\!\left(\bigcup_{i=1}^\infty A_i\right)=\sum_{i=1}^\infty\mu(A_i).
        \]
    \end{enumerate}
\end{itemize}
\end{definition}

A measure space is a rigorous way of assigning "size" or "volume" to sets, even in cases where standard geometry fails. For instance, on $\mathbb R^d$, Lebesgue measure generalizes the notion of length, area, and volume to very irregular sets.

\begin{definition}{Probability space}{}
A \emph{probability space} is a measure space $(X,\mathscr F,\mathbb P)$ where $\mathbb P(X)=1$.
\end{definition}

A probability space describes all possible outcomes (the sample space $X$), the events we can talk about (the $\sigma$-algebra $\mathscr F$), and how likely each event is (the measure $\mathbb P$).  

In context, every learning configuration $(S,R)$ induces a probability space:
\[
(\Theta,\mathscr B(\Theta), K(S,R)),
\]
where $K(S,R)$ acts as a probability measure over parameter states $\theta$. This makes $(\Theta,\mathscr B(\Theta),K(S,R))$ a valid probability space on which we can define and integrate measurable functions like $M_\Phi(\theta)$.


\begin{definition}{Borel probability measure}{}
Let $(X,\tau)$ be a topological space and let $\mathscr B(X)$ denote its Borel $\sigma$-algebra. A \emph{Borel probability measure} on $X$ is a measure $\mu:\mathscr B(X)\to[0,1]$ satisfying $\mu(X)=1$.

We denote by $\Prob(X)$ the set of all Borel probability measures on $X$.When $X$ is Polish, $(\Prob(X),\tau_{\mathrm{weak}})$ is also Polish (by Prokhorov’s theorem), making it the natural codomain for Markov kernels.
\end{definition}

A Borel probability measure assigns probabilities to all Borel subsets of a topological space.
It is the most natural way to extend the idea of “probability distributions” to general spaces:
instead of describing probability by densities or histograms,
a Borel measure simply says which subsets of $X$ have what probability mass.

All probability measures appearing in this theory—those produced by the training kernel $K(S,R)$, the predictors $\Pi_\theta$, and the data distribution $\mu$—are Borel probability measures.
This guarantees:
\begin{itemize}
    \item Compatibility with the topological structure of $\mathcal S$, $\mathcal R$, and $\Theta$ (Regularities B1–B2 and B7).
    \item Applicability of the Portmanteau theorem and weak topology on $\Prob(\Theta)$.
    \item Existence of integrals such as
          \[
          m_\Phi(S,R)
            = \int_\Theta M_\Phi(\theta)\,dK(S,R)(\theta),
          \]
          which are Lebesgue integrals with respect to a Borel probability measure.
\end{itemize}

\begin{definition}[Pushforward measure]
If $f:X\to Y$ is measurable and $\mu$ is a measure on $X$,  
the \emph{pushforward measure} $f_\#\mu$ on $Y$ is defined by
\[
f_\#\mu(B)=\mu(f^{-1}(B)),\qquad B\in\mathscr B(Y).
\]
\end{definition}

Given a probability space $(X,\mathscr A,\mu)$ and an integrable function $f:X\to\mathbb R$,  
the expectation is defined as
\[
\E_\mu[f] := \int_X f\,d\mu.
\]
This is the mathematical backbone of all expressions like  
$m_\Phi(S,R)=\E_{\theta\sim K(S,R)}[M_\Phi(\theta)]$.

\subsubsection{Markov Kernels and Weak Continuity}
\label{app:markov-kernel}

\begin{definition}{Markov kernel}{}
Let $(X,\mathscr A)$ and $(Y,\mathscr B)$ be measurable spaces.  
A function $K:X\times\mathscr B\to[0,1]$ is a \emph{Markov kernel} if:
\begin{enumerate}
    \item For each $A\in\mathscr B$, the map $x\mapsto K(x,A)$ is $\mathscr A$-measurable.
    \item For each $x\in X$, the map $A\mapsto K(x,A)$ is a probability measure on $(Y,\mathscr B)$.
\end{enumerate}
\end{definition}

It associates to each point $x\in X$ a probability measure on $(Y,\mathcal B)$ such that, for every measurable set $B\in \mathcal B$, the map $x\mapsto\kappa (B,x)$ is measurable with respect to the $\sigma$-algebra $\mathcal A$.

If a deterministic function gives you one point, a Markov kernel gives you a whole distribution. We can see kernels as a stochastic function: A Markov kernel is a stochastic map: instead of sending $x$ to a single point of $Y$, it sends $x$ to a probability distribution on $(Y,\B(Y))$ in a measurable way.

In practice, kernels let us say “the output is random, but in a controlled, measurable way," which is exactly why we use it in defining the training kernel. Without the kernel being measurable, the composed quantity
\[
(S,R)\mapsto \int_\Theta M_\Phi(\theta)\,dK(S,R)(\theta)
\]
would not be guaranteed measurable; this property is crucial in Lemma~\ref{lem:wellposed-cont}.

\begin{definition}{Weak continuity}{}
Let $\Theta$ be a Polish space with Borel $\sigma$-algebra $\mathscr B(\Theta)$.
A Markov kernel $K:(\mathcal S\times\mathcal R)\to \Prob(\Theta)$ is said to be
\emph{weakly continuous} if, for every sequence $(S_n,R_n)\to(S,R)$ in $\mathcal S\times\mathcal R$,
\[
K(S_n,R_n)\Rightarrow K(S,R)
\quad\text{in }(\Prob(\Theta),\tau_{\text{weak}}),
\]
i.e.\ for all bounded continuous $f:\Theta\to\mathbb R$,
\[
\int_\Theta f(\theta)\,dK(S_n,R_n)(\theta)
   \longrightarrow
\int_\Theta f(\theta)\,dK(S,R)(\theta).
\]
\end{definition}

Weak continuity ensures that small perturbations in the system or resource parameters produce only small changes (in the weak sense) in the resulting distribution of trained parameters. This is the formal notion behind the idea that "training behaves continuously" with respect to $(S,R)$.

This property connects the geometry of the learning domain $\mathcal S\times\mathcal R$ with the probabilistic space $\Prob(\Theta)$. It is the assumption used in Lemma~\ref{lem:wellposed-cont} to guarantee the continuity of $m_\Phi(S,R)$ via the Portmanteau theorem.

\noindent\textbf{Remark.}
Because $\mathcal S\times\mathcal R$ and $\Prob(\Theta)$ are Polish, sequential weak continuity is equivalent to topological continuity with respect to $\tau_{\mathrm{weak}}$.

\subsubsection{Weak Convergence and Topologies}
\label{app:weak-topology}
Reference: Billingsley (1968), Villani (2009).

Let $\Theta$ be a Polish space with Borel $\sigma$-algebra $\mathscr B(\Theta)$.
Denote by $\Prob(\Theta)$ the set of Borel probability measures on $\Theta$.

\begin{definition}{Weak convergence}{}
A sequence $(\mu_n)\subseteq \Prob(\Theta)$ is said to \emph{converge weakly} to $\mu\in\Prob(\Theta)$, written
$\mu_n\Rightarrow\mu$, if
\[
\int_\Theta f(\theta)\,d\mu_n(\theta)\longrightarrow
\int_\Theta f(\theta)\,d\mu(\theta)
\quad\text{for every bounded continuous }f:\Theta\to\mathbb R.
\]
\end{definition}

\begin{definition}{Weak Topology}{}
    The \textit{weak topology} is the coarsest topology such that, for every bounded continuous $f:\Theta\to \R$, the map
    \[
    \mu \longmapsto \int_\Theta f(\theta) \ d\mu(\theta)
    \]
    is continuous.
\end{definition}

So a sequence of distributions $(\mu_n)$ converges weakly to $\mu$ if every continuous measurement of the system sees the same averages in the limit. That is, even if individual samples from $\mu_n$ fluctuate wildly, their aggregate expectations on smooth observables stabilize. In context, $f(\theta)=M_\Phi(\theta)$ is one such observable, measuring capability. The weak topology ensures that small changes in $(S,R)$, hence in the distribution $K(S,R)$, can produce small changes in expected capabilities $m_\Phi(S,R)$.

\subsubsection{Portmanteau Theorem}
\label{app:portmanteau}

\begin{theorem}{Portmanteau}{}
For probability measures $\mu_n,\mu\in\Prob(\Theta)$ on a metric space $\Theta$, the following are equivalent:
\begin{enumerate}
    \item $\mu_n\Rightarrow\mu$.
    \item For all bounded continuous $f:\Theta\to\mathbb R$,
    \[
    \int f\,d\mu_n \to \int f\,d\mu.
    \]
    \item For every closed set $F\subseteq\Theta$,
    \[
    \limsup_{n\to\infty}\mu_n(F)\le\mu(F).
    \]
    \item For every open set $G\subseteq\Theta$,
    \[
    \liminf_{n\to\infty}\mu_n(G)\ge\mu(G).
    \]
\end{enumerate}
\end{theorem}

Weak convergence formalizes the idea that distributions are close when they give similar expectations to all smooth observables. It governs the continuity of integrals when the measures vary (contrast with the Dominated Convergence Theorem, which governs limits when the functions vary).

\begin{corollary}{Continuity of integrals under weak convergence}{}
If $\mu_n \Rightarrow \mu$ and $f$ is bounded and continuous, then $\int f\,d\mu_n \to \int f\,d\mu$.
\end{corollary}

This is important since for the training kernel
\[
K:(\mathcal S\times\mathcal R,\tau_\mathcal S\times\tau_\mathcal R)
   \longrightarrow (\Prob(\Theta),\tau_{\text{weak}}),
\]
and any bounded continuous capability function $M_\Phi:\Theta\to\mathbb R$,
the Portmanteau theorem implies
\[
K(S_n,R_n)\Rightarrow K(S,R)
\Longrightarrow
\int M_\Phi\,dK(S_n,R_n)\to \int M_\Phi\,dK(S,R),
\]
which yields the local continuity of
$m_\Phi(S,R)=\E_{\theta\sim K(S,R)}[M_\Phi(\theta)]$
established in Lemma 1 of the main text.

\subsubsection{Lebesgue Integration}
\label{app:lebesgue}

\begin{definition}[Lebesgue integral]
Let $(X,\mathscr A,\mu)$ be a measure space and $f:X\to[0,\infty]$ a measurable function.
The \emph{Lebesgue integral} of $f$ with respect to $\mu$ is defined as
\[
\int_X f\,d\mu = \sup\left\{
   \int_X s\,d\mu \; :\;
   s \text{ is a simple function with } 0\le s\le f
\right\},
\]
where a simple function is a finite linear combination
$s(x)=\sum_{i=1}^n a_i\mathbf 1_{A_i}(x)$ with $A_i\in\mathscr A$ and $a_i\ge0$.
For general (possibly signed) measurable $f$, we define
\[
\int f\,d\mu = \int f^+\,d\mu - \int f^-\,d\mu,
\]
where $f^+=\max(f,0)$ and $f^-=\max(-f,0)$.
\end{definition}

Lebesgue integration measures the "weighted area" under $f$ by first cutting the \emph{range} of $f$ into levels rather than partitioning the \emph{domain} as in Riemann integration. It is designed to handle discontinuities, limits, and arbitrary measurable domains robustly.

Riemann integration only works when the domain is an interval and $f$ is nearly continuous everywhere.  
Lebesgue integration, on the other hand, can integrate over arbitrary measurable sets and with respect to arbitrary measures. In our case, we integrate over $(\Theta,\mathscr B(\Theta),K(S,R))$, where $K(S,R)$ is a probability measure that may not have a density or any geometric structure. Thus, the integral
\[
\int_\Theta M_\Phi(\theta)\,dK(S,R)(\theta)\eqqcolon m_\Phi(S,R)
\]
is necessarily a \emph{Lebesgue integral.}

Known properties:
\begin{itemize}
    \item \textbf{Linearity:} $\int (a f + b g)\,d\mu = a\int f\,d\mu + b\int g\,d\mu$.
    \item \textbf{Monotone convergence:} If $0\le f_n\uparrow f$, then $\int f_n\,d\mu\uparrow\int f\,d\mu$.
    \item \textbf{Dominated convergence:} If $|f_n|\le g\in L^1(\mu)$ and $f_n\to f$ pointwise, then $\int f_n\,d\mu\to\int f\,d\mu$.
    \item \textbf{Absolute integrability:} If $\int |f|\,d\mu<\infty$, we write $f\in L^1(\mu)$.
\end{itemize}

Relevance to this theory:
\begin{itemize}
    \item All expectations, such as $\E_{\theta\sim K(S,R)}[M_\Phi(\theta)]$, are Lebesgue integrals on the probability space $(\Theta,\mathscr B(\Theta),K(S,R))$.
    \item Lebesgue integration allows us to handle continuous, discontinuous, or even infinite-dimensional parameter spaces uniformly.
    \item Theorems like Dominated Convergence (Appendix~A.x+2) guarantee that taking limits and integrating over $\Theta$ commute—crucial for proving continuity and scaling-law results.
\end{itemize}

For example, if $\Theta=\mathbb R^d$ and $K(S,R)$ has a density $p_{S,R}(\theta)$,  
then the Lebesgue integral reduces to the familiar expression
\[
m_\Phi(S,R)=\int_{\mathbb R^d} M_\Phi(\theta)\,p_{S,R}(\theta)\,d\theta,
\]
which is the Riemann integral weighted by the density, but the general Lebesgue form remains valid even when $p_{S,R}$ does not exist.

\subsubsection{Dominated Convergence Theorem}
\label{app:dct}

\begin{theorem}{Dominated Convergence}{}
Let $(X,\mathscr A,\mu)$ be a measure space and let $(f_n)_{n\in\mathbb N}$ be a sequence of measurable functions 
$f_n:X\to\mathbb R$ such that $f_n(x)\to f(x)$ for $\mu$-almost every $x\in X$.
Assume there exists an integrable function $g\in L^1(\mu)$ satisfying
\[
|f_n(x)|\le g(x) \quad \text{for all } n\in\mathbb N,\ \text{for $\mu$-a.e. }x\in X.
\]
Then $f$ is integrable and
\[
\lim_{n\to\infty}\int_X f_n\,d\mu=\int_X f\,d\mu.
\]
\end{theorem}

The theorem allows one to interchange limits and integrals when the functions themselves vary, provided that all are \textit{dominated} by a common integrable envelope $g$.
It ensures continuity of the map $f\mapsto \int f\,d\mu$ under pointwise convergence with uniform integrability.
In our setting this is used internally, e.g.\ when defining
\[
M_\Phi(\theta)=\E_{(x,y)\sim\mu}\E_{y'\sim \Pi_\theta(\cdot|x)}[U(y',y)],
\]
to justify passing limits through expectations as the inner functions vary.

\subsection{Topology and Functional Analysis}

\subsubsection{Topological Vector and Banach Spaces}
\label{app:tvs}

\subsubsection{Hausdorff and Polish Spaces}
\label{app:hausdorff-polish}

\begin{definition}[Hausdorff space]
A topological space $(X,\tau)$ is \emph{Hausdorff} if for any two distinct points $x_1,x_2\in X$, there exist open sets $U,V\in\tau$ such that
\[
x_1\in U,\quad x_2\in V,\quad U\cap V=\emptyset.
\]
\end{definition}

The Hausdorff property ensures that points can be separated by neighborhoods. In a Hausdorff space, limits of sequences (if they exist) are unique. This property is the minimal separation requirement for doing analysis. Without it, the notion of continuity or convergence becomes ambiguous.

The Hausdorff condition is built into most of our spaces—$\mathcal S$, $\mathcal R$, and $\Theta$—so that
limits of parameter sequences $\theta_n\to\theta$ or procedure sequences $(S_n,R_n)\to(S,R)$ are well-defined. It ensures the continuity statements in Lemma~\ref{lem:Mphi-cont} and Lemma~\ref{lem:mphi-cont} have an unambiguous sense of “approaching a unique limit.”


\begin{definition}[Polish space]
A \emph{Polish space} is a topological space that is separable and completely metrizable;
that is, there exists a metric $d$ on $X$ such that:
\begin{enumerate}
    \item $(X,d)$ is complete: every Cauchy sequence converges in $X$;
    \item $(X,d)$ is separable: there exists a countable dense subset $D\subseteq X$.
\end{enumerate}
\end{definition}

Polish spaces strike the perfect balance between richness and manageability: they are large enough to include infinite-dimensional and function spaces, but structured enough for all standard convergence and measure-theoretic results to hold. They are the canonical stage for probability theory and functional analysis.

Basic Regularity (B7) asserts that $\Theta$ is Polish. This guarantees:
\begin{itemize}
    \item Weak convergence of measures (used in B8 and Lemma~\ref{lem:mphi-cont}) is well-defined and metrizable.
    \item Compactness/tightness arguments can be applied when studying scaling limits.
    \item $\Prob(\Theta)$ is itself Polish, allowing kernels
          $K:(\mathcal S\times\mathcal R)\to\Prob(\Theta)$
          to be treated with standard continuity theorems.
\end{itemize}

In short, the “Polish” assumption ensures that our learning domain behaves like a well-structured infinite-dimensional manifold where probability, continuity, and convergence all coexist harmoniously.


\begin{definition}[Sequential continuity]
Let $(X,\tau_X)$ and $(Y,\tau_Y)$ be topological spaces and let $f:X\to Y$. The map $f$ is said to be \emph{sequentially continuous} at $x\in X$ if, for every sequence $(x_n)\subseteq X$ with $x_n\to x$ in $\tau_X$, we have
\[
f(x_n)\to f(x)\quad\text{in }\tau_Y.
\]
It is \emph{sequentially continuous} on $X$ if it is sequentially continuous at every $x\in X$.
\end{definition}

In general, sequential continuity is a weaker condition than topological continuity: it only checks the preservation of convergence for sequences, while full continuity requires the preimage of every open set in $Y$ to be open in $X$. However, in first-countable spaces—those whose topology can be captured by sequences—the two notions coincide.

\begin{proposition}[Equivalence in metric and Polish spaces]
If $X$ is a metric (hence Hausdorff and first-countable) space and $Y$ is any topological space, then a map $f:X\to Y$ is continuous if and only if it is sequentially continuous. In particular, this holds for all Polish spaces.
\end{proposition}

\begin{proof}[Proof sketch]
If $f$ is continuous, it is sequentially continuous by definition. Conversely, suppose $f$ is sequentially continuous.
In metric spaces, a set $U\subseteq X$ is open iff for every $x\in U$ there exists $\varepsilon>0$ such that $B_\varepsilon(x)\subseteq U$, which is equivalent to saying that no sequence from $X\setminus U$ can converge to $x$.
Sequential continuity ensures that the image of any convergent sequence preserves this property, so $f^{-1}(V)$ is open whenever $V$ is open in $Y$. Hence $f$ is continuous.
\end{proof}

In metric (and therefore Polish) spaces, sequences capture all possible modes of convergence. So verifying continuity via sequences is not merely sufficient—it is \textit{equivalent}. This equivalence is why proofs on Polish spaces often conclude with the line: \emph{“sequential continuity implies continuity.”}

In Lemma~\ref{lem:mphi-cont}, we showed that
\[
m_\Phi(S_n,R_n)\to m_\Phi(S,R)
\]
whenever $(S_n,R_n)\to(S,R)$ and $K(S_n,R_n)\Rightarrow K(S,R)$. This gives \emph{sequential} continuity of $m_\Phi$.
Because $(\mathcal S\times\mathcal R,\tau_\mathcal S\times\tau_\mathcal R)$ is Polish by Regularities (B1)--(B2),
sequential continuity immediately implies topological continuity. Thus the assumption that our learning spaces are Polish is what allows the proof to close cleanly.

The equivalence between sequential and topological continuity is more than a technicality—it expresses the deep regularity of the spaces your theory operates on. Because your learning domains are Polish, you can safely reason about convergence entirely in terms of sequences (as we did in Lemma~\ref{lem:mphi-cont}), without invoking nets or filters. This makes the theory operationally elegant and aligns it with the way machine-learning processes actually behave in practice: training procedures, parameter updates, and data limits are all naturally sequential.

\subsubsection{Function-Space Topologies}
\label{app:function-space-topologies}

\begin{definition}{Pointwise convergence topology}{}
Let $Y^X$ denote the set of all functions $f:X\to Y$, where $(Y,\tau_Y)$ is a topological space.
The \emph{pointwise convergence topology} on $Y^X$ is the coarsest topology such that,
for every $x\in X$, the evaluation map
\[
\mathrm{ev}_x : f \mapsto f(x)
\]
is continuous.
A sequence $(f_n)$ converges to $f$ pointwise if $f_n(x)\to f(x)$ for each $x\in X$.
\end{definition}

\begin{definition}{Compact-open topology}{}
Let $(X,\tau_X)$ be a topological space and $(Y,\tau_Y)$ a metric space.
The \emph{compact-open topology} on $C(X,Y)$, the space of continuous maps $X\to Y$, is generated by subbasic sets of the form
\[
W(K,U)=\{\,f\in C(X,Y):f(K)\subseteq U\,\},
\]
where $K\subseteq X$ is compact and $U\subseteq Y$ is open.
\end{definition}

Many objects in the theory are function-valued:
\begin{itemize}
    \item the predictor $\Pi_\theta:\mathcal X\to\Prob(\mathcal Y)$,
    \item the training map $K:(\mathcal S\times\mathcal R)\to\Prob(\Theta)$,
    \item the capability function $M_\Phi:\Theta\to\R$.
\end{itemize}
Each lives naturally in a function space. By equipping these spaces with appropriate function topologies, we can speak rigorously of continuity and convergence of mappings between them.

Specifically:
\begin{itemize}
    \item The continuity of $\theta\mapsto\Pi_\theta$ in (B4) is continuity
          in a weak function-space topology on $\Prob(\mathcal Y)$.
    \item The continuity of $(S,R)\mapsto K(S,R)$ in (B8) is continuity
          in the function-space topology on $\Prob(\Theta)$ induced by the weak topology.
    \item Lemma~\ref{lem:Mphi-cont} and Lemma~\ref{lem:mphi-cont}
          rely on these topologies to interpret weak convergence and ensure that
          expectations vary smoothly under parameter and resource perturbations.
\end{itemize}

Thus, function-space topologies give the geometric foundation for treating learning systems as continuous transformations between probability spaces.

\subsubsection{Wasserstein Metric and Optimal Transport}
\label{app:w-metric}

Distributions are "close" if we can rearrange one into the other with little effort.

Formally, let $(X,d)$ be a metric space. Let $\mathcal P _p(X)$ be the set of Borel probability measures $\mu$ on $X$ with finite $p$-th moment:
\[
    \int_X d(x,x_0)^p\mu(dx)<\infty
\]
for some arbitrary reference point $x_0\in X$.

For $\mu, \nu\in \mathcal P _p(X)$, a \textit{coupling} is a joint distribution $\gamma$ on $X\times X$ with marginals $\mu$ and $\nu$:

\[
    \gamma(A\times X)=\mu (A),\quad \gamma(X\times B)=\nu(B)
\]

for all measurable $A,B\subseteq X$

Then, the \textit{Wasserstein-}$p$ distance is:
\[
    W_p(\mu,\nu)=\left(\inf_{\gamma\in \Gamma(\mu,\nu)}\int_{X\times X} d(x,y)^p d\gamma(x,y)\right)^{1/p}
\]
where $\Gamma(\mu, \nu)$ is the set of all couplings.

So 
\subsubsection{Kantorovich–Rubinstein Duality}
\label{app:kantorovich–rubinstein-duality}

\subsubsection{Product and Compact-Open Topologies}
\label{app:open-compact-topologies}

\subsubsection{First-countability and sequential continuity}

\subsubsection{The Dual Space}

\subsubsection{The General Linear Group}

\subsubsection{Linear Functionals}

\subsubsection{Affine Hyperplanes}

\subsubsection{Affine reparameterization and gauge freedom}

\subsubsection{Continuity of Functionals and Measurable Maps}
\label{app:continuity-of-functionals}

\subsection{Algebraic and Structural Foundations}

\subsubsection{Equivalence Relations and Classes}
\label{app:equivalence-relations}

\subsubsection{Quotient Structures}
\label{app:equivalence-relations}

\subsubsection{Partial Orders and Resource Topologies}
\label{app:partial-order}

\subsubsection{Monoids and Homomorphisms}
\label{app:monoids}

\subsubsection{Symmetry and Group Actions}
\label{app:equivalence-relations}





\subsubsection{Category-Theoretic Remark}
\label{app:partial-order}

\subsection{Statistics and Learning Theory}

\subsubsection{Order Parameters and Phase Transitions}
\label{app:order-parameter}
Reference: Stanley (1971).

\subsubsection{Lipschitzness}
\label{app:lipschitzness}
Reference: Heinonen (2005).

\section{Early Empirical Prototypes}

\subsection{Shared setup (for all experiments)}

\begin{itemize}
    \item Systems And procedures: keep a fixed procedure $P$ (AdamW + cosine schedule, bf16, attention-only transformer, tied embeddings) and define your procedure-local neighborhood via your procedure distance with hard categoricals and tolerance-weighted Euclidean on continuous knobs (±10% LR/WD, ±0.02 dropout, etc.). This makes A3’s “locality” operational.
    \item Training map $K$ and predictor $\Pi_\theta$: compute 
    \[
    m_\Phi(S,R)=\mathbb E_{\theta\sim K(S,R)}[M_\Phi(\theta)]
    \]
    with continuous scoring $\mathcal U$ (probability mass on the acceptance set or normalized log-prob), not thresholds (your A6-style precaution).
    \item \textbf{Resources:} start with $R=(N,P,T)$ and $\psi(R)=\log R$; later add a 4th coordinate (context length $L$, or quality $Q$).
    \item \textbf{Grid:} small, dense patches beat huge sweeps. Typical 18–36 runs per capability: $P\!\in\!\{0.5,1,2\}$M; $N\!\in\!\{1,2,4\}$M tokens; steps so tokens $\approx N$; 2 seeds each.
    \item \textbf{Models:} 1–4 layer attn-only transformer, context 64–128, tiny vocab. Save activations for probe tests.
    \item \textbf{Analysis:} compute collapse error CE and optimized CE versus (i) single-knob baselines and (ii) a smooth 2-D/3-D surface $g(\psi(R))$ (thin-plate spline), with cross-validation. This is exactly your A2 “Curve Fit Quality” test.
\end{itemize}

\subsection{A1 Monotone capability—try to induce non-monotonicity}

\textbf{Goal:} Show $m_\Phi(S,R)$ can decrease in a single resource coordinate, contradicting A1’s coordinate-wise monotonicity.

\textbf{Setup:} Capability $\Phi$: parity (binary string → even/odd). Score: prob(correct).

\textbf{Protocol:}
\begin{itemize}
    \item Fix $N,T$. Sweep $P$ across the interpolation threshold (classic double-descent regime) with small label noise (e.g., 10%).
    \item Separately, fix $N,P$ and sweep $T$ far past convergence (no early stopping).
    \item \textbf{Fail A1} if seed-averaged $m_\Phi$ dips as $P $ grows (double descent persists under continuous scoring), or decreases with $T$ as you keep training (controlled overfitting), beyond noise CIs. If you observe dips only with thresholded metrics but not continuous scores, you’ve exposed a “mirage” (supports your continuous-score stance).
\end{itemize}

\subsubsection{Findings}

For the first experiment I trained a 3-layer MLP (1 input, 1 hidden, 1 output) with Xavier initialization, an Adam optimizer, Cross-Entropy loss

$\text{seeds} = (0,1,2,3,4,5,6,8,9,10)$
Other resources not included in $R$:
$L\text{ (input length) }=8$
$\mathrm{sweeps}_p \text{ (label noise)}=0.0$
$\text{lr (learning rate)}=0.01$
$\text{batch size}=128$

\begin{figure}[h]
    \centering
    \includegraphics[width=0.65\textwidth]{A1_parity/parity_N.png}
    \caption{Capability $m_\Phi$ vs.~training tokens $T$. Error bars represent $\pm$1 std.~dev across seeds.}
    \label{fig:mphi-scaling}
\end{figure}

\begin{table}[h]
\centering
\begin{tabular}{rccc}
\toprule
$N$ (training size) & $m_\Phi$ & $\pm$ std.~dev. & Notes \\
\midrule
10  & 0.4959655 & 0.009785 & abccc \\
50  & 0.528840 &  0.020138 & abccc \\
100 & 0.600190 & 0.022990 & abccc \\
250 & 0.810683 & 0.028369 & abccc \\
500 & 0.947869 & 0.011377 & abccc \\
1000 & 0.994044 & 0.003330 & abccc \\
2000 & 0.998997 & 0.000836 & abccc \\
5000 & 0.998513 & 0.002875 & abccc \\
10000 & 0.999188 & 0.000308 & abccc \\
20000 & 0.999216 & 0.000561 & abccc \\

\bottomrule
\end{tabular}
\caption{Seed-averaged capability scores $m_\Phi$ as a function of training size $N$.}
\label{tab:mphi-results}
\end{table}
\clearpage

\begin{figure}[h]
    \centering
    \includegraphics[width=0.65\textwidth]{A1_parity/parity_P.png}
    \caption{Capability $m_\Phi$ vs.~hidden width $P$. Error bars represent $\pm$1 std.~dev across seeds.}
    \label{fig:mphi-scaling}
\end{figure}

\begin{table}[h]
\centering
\begin{tabular}{rccc}
\toprule
$N$ (training size) & $m_\Phi$ & $\pm$ std.~dev. & Notes \\
\midrule
2  & 0.568036 & 0.059987 & abccc \\
4  & 0.704992 &  0.103550 & abccc \\
8 & 0.893279 & 0.056577 & abccc \\
12 & 0.973216 & 0.016356 & abccc \\
16 & 0.989956 & 0.009209 & abccc \\
24 & 0.996192 & 0.004893 & abccc \\
32 & 0.998997 & 0.000836 & abccc \\
48 & 0.999625 & 0.000100 & abccc \\
64 & 0.999681 & 0.000316 & abccc \\
128 & 0.999885 & 0.000025 & abccc \\
\bottomrule
\end{tabular}
\caption{Seed-averaged capability scores $m_\Phi$ as a function of training size $N$.}
\label{tab:mphi-results}
\end{table}
\clearpage

\begin{figure}[h]
    \centering
    \includegraphics[width=0.65\textwidth]{A1_parity/parity_T.png}
    \caption{Capability $m_\Phi$ vs.~training steps $T$. Error bars represent $\pm$1 std.~dev across seeds.}
    \label{fig:mphi-scaling}
\end{figure}

\begin{table}[h]
\centering
\begin{tabular}{rccc}
\toprule
$N$ (training size) & $m_\Phi$ & $\pm$ std.~dev. & Notes \\
\midrule
10  & 0.501841 & 0.000950 & abccc \\
30  & 0.506235 &  0.003317 & abccc \\
50 & 0.520094 & 0.011847 & abccc \\
75 & 0.574808 & 0.040671 & abccc \\
100 & 0.667526 & 0.064508 & abccc \\
150 & 0.846540 & 0.047339 & abccc \\
300 & 0.977447 & 0.008803 & abccc \\
500 & 0.995723 & 0.001264 & abccc \\
1000 & 0.998997 & 0.000836 & abccc \\
3000 & 0.999773 & 0.000736 & abccc \\

\bottomrule
\end{tabular}
\caption{Seed-averaged capability scores $m_\Phi$ as a function of training size $N$.}
\label{tab:mphi-results}
\end{table}
\clearpage

In this first experiment, I attempted to test Axiom 1 (monotonicity of capability with respect to resources) on the parity capability using multilayer perceptrons (MLPs). I systematically varied all three resource coordinates: the number of training examples $N$, hidden width $P$, and training steps $T$. I also explored different depths, activation functions, learning rates, and data sizes. Across all settings, the seed-averaged capability score $m_\Phi$ remained flat at chance level ($ \approx 0.5$) with only random noise fluctuations, regardless of how resources were scaled. Consequently, the resulting plots were effectively random up-and-down wiggles with no interpretable monotonic trend.

This shows that for parity at input length $L=32$, the MLP family $F$ I tested is unable to represent the capability at all: increasing resources does not move performance above chance. What initially looked like a falsification of Axiom 1 is better interpreted as a gap in the axiom’s formulation. Monotonicity should not be assumed universally; it only makes sense once the capability is within the representational reach of the system $S$. In other words, Axiom 1 requires a precondition: if a system cannot express a capability, scaling resources does not guarantee improvement. This is what I talk more about in the next experiment.

The parity experiment thus provides my first concrete evidence that monotonicity axioms must be conditional on representability. While the immediate plots are uninformative, the exercise clarified a critical theoretical direction: embedding representability into the axioms themselves so that empirical results align naturally with the formal framework.

\clearpage

\subsection{Validity of Representability}

This is the experiment I did to arrive at a definition of representability.

\clearpage

\subsection{A5: Testing local continuity between procedures and the learning system}

\subsubsection{Overview}

In this experiment we fix data $D$ and resources $R$, and we jitter the procedure $P$ (learning rate, width, seed, etc), giving us multiple procedures $P_1,P_2,\dots$, and in turn multiple learning systems $P_1\times D, P_2\times D,\dots$. We then train each learning system once (sampling a $\theta$ from the training kernel), and compare the induced precitor maps $\Pi_\theta$ via their capacity score $M_\Phi(\theta)$.

By axiom 5, it should be the case that varying the procedure slightly within its neighborhood in $(\mathcal P, \tau_\mathcal P)$, the emergent capability function (parity), $m_\Phi((D,P),R)$ varies continuously with respect to those perturbations. This is what we are testing. 

So we empirically want to "prove" the continuity of the deterministic trained predictor map
\[
G:(P, \omega)\mapsto \Pi_{\theta(P, \omega)} \in \mathcal S _{\text{pred}}
\]
where $\omega$ is the randomness/seed. In other words, we check whether small procedure changes implies small changes in the predictor, for single seeds.

\clearpage
\subsubsection{Findings}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.65\textwidth]{figures/A5/local continuity/function-space distance.png}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.65\textwidth]{figures/A5/local continuity/m_phi function distance.png}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.65\textwidth]{figures/A5/local continuity/procedure-space distance.png}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.65\textwidth]{figures/A5/local continuity/procedure distance base procedures.png}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.65\textwidth]{figures/A5/local continuity/representation-space distance.png}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.65\textwidth]{figures/A5/local continuity/cka distance vs procedure distance.png}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.65\textwidth]{figures/A5/local continuity/linear kernel vs function distance.png}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.65\textwidth]{figures/A5/local continuity/rbf kernel vs function distance.png}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.65\textwidth]{figures/A5/local continuity/mds embedding by function distance.png}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.65\textwidth]{figures/A5/local continuity/mds embedding by cka distance.png}
\end{figure}

I also did an openness test for $R_\tau$ where $\tau = 0.90$:
\begin{itemize}
    \item $|R_\tau|$ (by lower CI): 8/8
    \item openness radius (median / mean $\pm$ sd): $0.1278 / 0.1309 \pm  0.0191$
\end{itemize}

Seed-avg Lipschitz $L$ (func): $7.44e-06$, $R^2=0.472$

\begin{figure}[h]
    \centering
    \includegraphics[width=0.65\textwidth]{A5/local continuity/local lipshitz for m_phi function distance.png}
\end{figure}

\clearpage
\subsubsection{Conclusions}

Thus, we've gathered concrete evidence that in function space the predictor varies continuously and very weakly under local procedure perturbations. So this experiment justifies the "nearby procedures leads to nearby systems" intuition. Note that this is a local continuity result for $G$, not yet for $K$, the training map.

Axiom 5 is an assumption of structure, and not a continuity claim by itself. To motivate it further we want to show that reasonable metrics/topologies on $\mathcal S$ lead to reasonable neighborhoods in $\mathcal S$. Tiny jitters in some $P$ or $D$ shouldn't cause wild, discontinuous jumps in the trained learning system's behavior.

The current results already supports this specifically \textit{for outputs}: 
\[
\text{tiny } d_X\Longrightarrow d_\text{func} \approx 10^{-5}
\]

That is, nearby procedures induce almost identical predictions. 95-percentage distances inside small $d_X$ balls are $\sim 10^{-5}$. Conclusively, the experiment supports using the proposed $d_X$ to topologize $\mathcal P$ and support local continuity of the procedure-to-system map in outputs. 

\subsubsection{Representation-space continuity}

During the same experiment, we don't have reasonable evidence to conclude anything about the representation space. It is inconclusive for representation geometry and does not yet test the stochastic training kernel continuity. The $1-\text{CKA}$ is much larger, with median $\sim 0.5-0.6$, and highly variable even under the tiny procedure changed made. So nearby procedures might yield representionally different learning systems. 

Secondly, simple linear fits give tiny positive slopes but negative $R^2$ for both spaces. There’s some monotone relationship with procedure distance, but high variance means we cannot draw a quantitative law yet.

Lightbulb!!! Right now Axiom 5 is very general, and while it is hard to justify in it of itself, this experiment might suggest that this axion is empirically well-motivated only at the level of predictor outputs. For representations, continuity is not guaranteed.

\subsection{testing Axiom 2: One-Dimension Collapse; Scaling Experiment}

\subsubsection{Background}

Axiom 2 formalizes the idea that emergence and capability scaling are governed by an intrinsic one-dimensional manifold in resource space - a law of “effective scaling.”
It predicts that as the number of parameters (P), tokens (T), and optimization steps (S) increase jointly, the model’s measured capability should rise monotonically following a sigmoidal pattern when plotted against the correct effective resource axis.

Our experiment aims to empirically test this claim on a simple controlled task: modular addition learning using MLPs of varying parameter count, token count, and training steps.
This task is deliberately simple so that we can isolate the structural regularities of emergence itself, independent of complex architecture-specific dynamics.

\subsubsection{Experimental Setup}

Each run trains a multilayer perceptron (ModAddMLP) on the modular addition task.
The model, data, and compute resources are characterized by:

\begin{itemize}
    \item $P$: number of trainable parameters, controlled by the model’s width and depth.
    \item $T$: number of training tokens (total data examples seen).
    \item $S$: number of training steps or optimization iterations.
\end{itemize}

For each configuration $(P,T,S)$, we train the model to convergence (or a fixed step budget) and record the final test accuracy test $A_\text{test}$, which we treat as an empirical estimate of capability,

\[
C=A_\text{test}\in [0,1]
\]

All results are logged into per-run directories (e.g., runs/P3000_T10000_S5000_seed0), and after averaging across random seeds, we obtain approximately 80 unique averaged points $(P,T,S,C)$ that populate our resource–capability landscape.

To test the axiom, we search for a direction $\alpha = (\alpha_P, \alpha_T, \alpha_S)$ in log-space such that capabilities collapse to a one-dimensional curve.
\begin{enumerate}
    \item We take logs of all resources:
    \[
        X=[\log P, \log T, \log S]
    \]
    \item we learn $\alpha$, the onset $s^*$, width $w$, and the parameters of a logistic function
    \[
    f(s')=C_\min + ( C_\max -C_\min )\sigma(s'),\quad \sigma(s')=\frac 1 {1+e^{-s'}}
    \]
    such that the mean-squared error between predicted and observed capabilities is minimized:
    \[
    \min_{\alpha, s^*,w,C_\min, C_\max} \E _{(P,T,S)}\left[\left( C-f\left( \frac{\alpha^\top \log R - s^*}{w}\right)\right)^2\right]
    \]
    \item During optimization, $\alpha$ is normalized ($\|\alpha\|_2=1$) to make its components interpretable as relative importance weights of the three resources.
\end{enumerate}

The fitted parameters define:
\begin{itemize}
    \item $\alpha$: direction of effective resource.
    \item $s^*$: onset of emergence (the log-resource where capability ≈ 0.5).
    \item $w$: sharpness of transition (the log-width between 10\% and 90\% capability $≈ 4.4 w$).
    \item $C_\min, C_\max$: lower and upper asymptotes of observed capability.
\end{itemize}

Finally, we compute collapse errors in both capability space and logit space (RMSE and MAE), and produce three diagnostic plots:

\begin{itemize}
    \item Capability vs projected resource (collapse curve).
    \item Predicted vs observed capabilities (parity).
    \item Residuals vs projected coordinate (structure of deviations).
\end{itemize}

\subsubsection{Results}

The fitted logistic function explains the majority of variation in capability across all 80 points.
The estimated direction and parameters are:

\begin{table}[h]
\centering
\begin{tabular}{rccc}
\toprule
Parameter & Meaning & Estimate \\
\midrule
$\alpha_P$  & param importance & 0.285  \\
$\alpha_T$  & token importance &  0.722 \\
$\alpha_S$ & step importance & 0.630 \\
$s^*$ & onset (log scale) & 15.125 \\
$w$ & width & 0.190 \\
\bottomrule
\end{tabular}
\caption{Estimated .}
\label{tab:mphi-results}
\end{table}
\clearpage

This implies that a capability primarily scales with data tokens and training steps, with parameter count having a smaller but non-negligible effect. A 1\% increase in token count is approximately equivalent to a 2.5\% increase in parameters along iso-capability contours. The transition from 10\% to 90\% capability occurs over a multiplicative increase of $\approx 2.3$ times in the effective resource.

Fit quality:

RMSE(capability) = 0.165

MAE(capability) = 0.075
→ Average error ≈7.5 pp capability.

RMSE(logit) = 5.05

MAE(logit) = 3.52

This means the logistic function captures the overall S-shaped trend, though it slightly overestimates capability near onset (as seen from residuals).

Collapse Plot: Points align along a smooth sigmoidal curve, confirming a 1-D monotone structure.
Deviations are systematic (underperformance near the lower shoulder), suggesting mild asymmetry rather than random noise.

Parity Plot: Predicted vs observed capability falls near the diagonal, especially in the high-capacity regime (above 0.7), indicating a stable mapping.

Residual Plot:
Negative residuals at small $z$ show the empirical curve rises more slowly than a perfect logistic — consistent with a Gompertz-like skew or a finite-size correction.

\subsubsection{Interpretation}

The empirical evidence supports Axiom 2 in its essential qualitative content:

Existence of a 1-D effective resource variable:
The discovery of a direction $\alpha$ in log-resource space along which all measured capabilities collapse provides strong evidence that the system’s behavior can be summarized by a single “effective resource” dimension.

Smooth monotonic relationship:
The capability rises monotonically and saturates smoothly — a fundamental characteristic of emergence.

Sigmoidal shape:
The observed functional form is well-approximated by a logistic curve, matching the axiom’s requirement of a smooth increasing function $f$.

Finite-size deviations:
Residual structure near onset reflects mild departures from pure logisticity, likely due to finite sampling or unmodeled nonlinear interactions.

Hence, the experiment empirically confirms the structural content of Axiom 2 — the existence of a low-dimensional smooth law governing the transition from non-representability to full representability of a capability.
It does not yet fix the exact shape of $f$, but validates its existence.

\subsubsection{Limitations}

While the evidence supports the axiom, several caveats remain:

The logistic function is a parametric assumption. The data suggest a slightly skewed curve; a non-parametric or generalized sigmoid (Gompertz/Richards) may improve fit.

The number of data points ($\approx 80$) is moderate but not large; bootstrapped confidence intervals are wide.

The task (modular addition) is simple; generalization to other cognitive or perceptual tasks remains untested.

The residuals show structure, suggesting small interaction effects between parameters, tokens, and steps not captured by a single $\alpha$.

Power-law exponents in the asymptotic tail remain uncertain due to the additive form of the resource aggregator.

Thus, these results should be interpreted as first-order empirical confirmation of Axiom 2’s structure, not as a final statistical proof.

Future improvements will aim to strengthen the validity and generality of Axiom 2 by:

Statistical Refinement:

Increase the number of runs per grid region ($\ge 300$ points) for more stable estimation.

Use non-parametric monotone regression to confirm the existence of a smooth mapping $f$ without assuming logistic form.

Quantify uncertainty in$\alpha, s^*,w$ using bootstrap or Bayesian inference, and evaluate confidence intervals.

Model \& Task Generalization:

Replicate the experiment on distinct tasks (e.g., parity, sorting, simple algorithmic reasoning).

Evaluate whether the same 1-D collapse structure holds, with possibly task-specific $\alpha$ and $f$.

Theoretical Extensions:

Derive closed-form predictions for the scaling law coefficients assuming the axioms hold jointly.

Test if Axiom 2 combined with resource constraints (Axiom 3) reproduces known empirical scaling laws (e.g., Chinchilla, OpenAI power laws).

Asymptotic Tail:

Redefine the tail aggregator multiplicatively and refit the exponents $a,b,c,\gamma$.

Compare derived $\gamma$ with known theoretical values from scaling literature (e.g., Hestness et al. 2017, Kaplan et al. 2020).

Cross-Task Regularities:

Evaluate if $\alpha$ (direction weights) remain approximately invariant across architectures and tasks — which would hint at a universal scaling manifold.

% References
\newpage
\bibliographystyle{abbrvnat}
\bibliography{references}

\end{document}
